{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Libraries for FFKAN Model\n",
    "\n",
    "This cell imports libraries for the **Fast Fourier KAN (FFKAN)** implementation:\n",
    "\n",
    "- **PyTorch**: Deep learning framework\n",
    "- **Standard ML libraries**: NumPy, Pandas, Scikit-learn\n",
    "- **Visualization**: Matplotlib, Seaborn\n",
    "\n",
    "**What is FFKAN?**\n",
    "Fast Fourier KAN is a variant of Kolmogorov-Arnold Networks that uses **Fourier features** instead of B-splines for learning activation functions. This approach:\n",
    "- Uses sine and cosine basis functions\n",
    "- Can be more computationally efficient\n",
    "- Better captures periodic patterns\n",
    "- Particularly suited for time-series data\n",
    "\n",
    "**Key Advantage**: Fourier basis functions are well-suited for modeling periodic phenomena like daily solar output patterns.\n",
    "\n",
    "**References:**\n",
    "- [Fourier Features Let Networks Learn High Frequency Functions](https://arxiv.org/abs/2006.10739)\n",
    "- [Fast Fourier KAN Implementation](https://github.com/Aaveshlabtech/FFTKAN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9779d675",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Requires: pip install pykan\n",
    "\n",
    "import os\n",
    "import seaborn as sns\n",
    "import datetime as datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define NaiveFourierKANLayer Implementation\n",
    "\n",
    "This cell contains the **NaiveFourierKANLayer** class implementation - a neural network layer that uses Fourier coefficients instead of traditional weights.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Fourier Series**: Represents functions as sums of sine and cosine terms\n",
    "- **Global Basis**: Unlike splines (local), Fourier functions are global and periodic\n",
    "- **Smooth Initialization**: Attenuates high-frequency components for stable training\n",
    "\n",
    "**How It Works:**\n",
    "1. For each input, computes sin(kx) and cos(kx) for k=1 to gridsize\n",
    "2. Multiplies by learned Fourier coefficients\n",
    "3. Sums across all frequencies and input dimensions\n",
    "\n",
    "**Advantages:**\n",
    "- Periodic functions naturally bounded (no out-of-grid issues)\n",
    "- Dense representation (global coverage)\n",
    "- Efficient for time-series and periodic data\n",
    "\n",
    "**Implementation Details:**\n",
    "- `gridsize`: Number of Fourier frequency terms\n",
    "- `smooth_initialization`: Attenuates high frequencies (k²) for stability\n",
    "- Normalization ensures unit variance output\n",
    "\n",
    "**Reference:**\n",
    "- [Fourier Features for Neural Networks](https://arxiv.org/abs/2006.10739)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf102a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'big_project'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbig_project\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msandbox\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfftKAN\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NaiveFourierKANLayer\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'big_project'"
     ]
    }
   ],
   "source": [
    "class NaiveFourierKANLayer(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Fourier-based KAN layer using sine and cosine basis functions.\n",
    "    \n",
    "    This layer replaces traditional linear transformations with learnable\n",
    "    Fourier series, enabling the network to learn complex non-linear functions.\n",
    "    \"\"\"\n",
    "    def __init__(self, inputdim, outdim, gridsize, addbias=True, smooth_initialization=False):\n",
    "        super(NaiveFourierKANLayer, self).__init__()\n",
    "        self.gridsize = gridsize\n",
    "        self.addbias = addbias\n",
    "        self.inputdim = inputdim\n",
    "        self.outdim = outdim\n",
    "        \n",
    "        # With smooth_initialization, fourier coefficients are attenuated by the square of their frequency.\n",
    "        # This makes KAN's scalar functions smooth at initialization.\n",
    "        # Without smooth_initialization, high gridsizes will lead to high-frequency scalar functions,\n",
    "        # with high derivatives and low correlation between similar inputs.\n",
    "        grid_norm_factor = (torch.arange(gridsize) + 1)**2 if smooth_initialization else np.sqrt(gridsize)\n",
    "        \n",
    "        # The normalization has been chosen so that if given inputs where each coordinate is of unit variance,\n",
    "        # then each coordinate of the output is of unit variance independently of the various sizes\n",
    "        self.fouriercoeffs = torch.nn.Parameter(\n",
    "            torch.randn(2, outdim, inputdim, gridsize) / (np.sqrt(inputdim) * grid_norm_factor)\n",
    "        )\n",
    "        if self.addbias:\n",
    "            self.bias = torch.nn.Parameter(torch.zeros(1, outdim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass computing Fourier series for each input.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor of shape (..., inputdim)\n",
    "            \n",
    "        Returns:\n",
    "            Output tensor of shape (..., outdim)\n",
    "        \"\"\"\n",
    "        xshp = x.shape\n",
    "        outshape = xshp[0:-1] + (self.outdim,)\n",
    "        x = torch.reshape(x, (-1, self.inputdim))\n",
    "        \n",
    "        # Starting at 1 because constant terms are in the bias\n",
    "        k = torch.reshape(torch.arange(1, self.gridsize + 1, device=x.device), (1, 1, 1, self.gridsize))\n",
    "        xrshp = torch.reshape(x, (x.shape[0], 1, x.shape[1], 1))\n",
    "        \n",
    "        # Compute sine and cosine terms\n",
    "        c = torch.cos(k * xrshp)\n",
    "        s = torch.sin(k * xrshp)\n",
    "        \n",
    "        # Compute the interpolation of the various functions defined by their Fourier coefficients\n",
    "        # for each input coordinate and sum them\n",
    "        y = torch.sum(c * self.fouriercoeffs[0:1], (-2, -1))\n",
    "        y += torch.sum(s * self.fouriercoeffs[1:2], (-2, -1))\n",
    "        \n",
    "        if self.addbias:\n",
    "            y += self.bias\n",
    "            \n",
    "        y = torch.reshape(y, outshape)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Feature Visualization Function\n",
    "\n",
    "Similar to the KAN notebook, this function visualizes learned feature relationships:\n",
    "\n",
    "**Key Difference from KAN:**\n",
    "FFKAN learns Fourier series representations instead of splines:\n",
    "- **Splines (KAN)**: Piecewise polynomial curves\n",
    "- **Fourier (FFKAN)**: Sums of sine and cosine functions\n",
    "\n",
    "**Visualization Benefits:**\n",
    "- Shows how each feature affects solar output\n",
    "- Reveals periodic patterns (important for solar/time data)\n",
    "- Provides model interpretability\n",
    "\n",
    "**Mathematical Background:**\n",
    "Fourier series can approximate any periodic function:\n",
    "`f(x) = a₀ + Σ[aₙcos(nx) + bₙsin(nx)]`\n",
    "\n",
    "**Reference:**\n",
    "- [Fourier Series Explanation](https://en.wikipedia.org/wiki/Fourier_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8e5985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def plot_kan_feature(model, feature_name, feature_cols, X_train, scaler_X):\n",
    "    \"\"\"\n",
    "    Plots the specific learned function (Physical Law) for a given feature.\n",
    "    \n",
    "    Args:\n",
    "        model: The trained KAN model\n",
    "        feature_name (str): The exact name of the column to plot (e.g. 'Wind Cooling')\n",
    "        feature_cols (list): The list of all feature names used in training\n",
    "        X_train (numpy array): The RAW (unscaled) training data (for calculating means/ranges)\n",
    "        scaler_X (StandardScaler): The fitted scaler used to transform the data\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Find the index\n",
    "    try:\n",
    "        feature_index = feature_cols.index(feature_name)\n",
    "    except ValueError:\n",
    "        print(f\"❌ ERROR: '{feature_name}' not found in feature list.\")\n",
    "        return\n",
    "\n",
    "    # 2. Setup Dimensions\n",
    "    input_dim = X_train.shape[1]\n",
    "    \n",
    "    # 3. Create Synthetic Data (Base = Mean of everything)\n",
    "    # We use 100 points for a smooth curve\n",
    "    dummy_input_raw = np.zeros((100, input_dim))\n",
    "    \n",
    "    for i in range(input_dim):\n",
    "        # Fill every column with its AVERAGE value from the real world\n",
    "        # This isolates the feature we care about\n",
    "        dummy_input_raw[:, i] = np.mean(X_train[:, i])\n",
    "\n",
    "    # 4. Vary the Target Feature\n",
    "    min_val = np.min(X_train[:, feature_index])\n",
    "    max_val = np.max(X_train[:, feature_index])\n",
    "    seq_values = np.linspace(min_val, max_val, 100)\n",
    "    \n",
    "    dummy_input_raw[:, feature_index] = seq_values\n",
    "\n",
    "    # 5. Scale & Predict\n",
    "    # The model speaks \"Scaled\", so we translate our raw numbers\n",
    "    dummy_input_scaled = scaler_X.transform(dummy_input_raw)\n",
    "    dummy_tensor = torch.from_numpy(dummy_input_scaled).float().to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(dummy_tensor).cpu()\n",
    "    \n",
    "    # Flatten for plotting\n",
    "    preds_flat = predictions.numpy().flatten()\n",
    "\n",
    "    # 6. Plot\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(seq_values, preds_flat, color='#007acc', linewidth=3, label='KAN Learned Law')\n",
    "    \n",
    "    # Formatting\n",
    "    plt.title(f\"Effect of {feature_name}\", fontsize=14)\n",
    "    plt.xlabel(f\"{feature_name} (Physical Units)\", fontsize=12)\n",
    "    plt.ylabel(\"Clearsky Index (Efficiency)\", fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add Reference Line\n",
    "    plt.axhline(y=1.0, color='red', linestyle='--', alpha=0.3, label=\"100% Efficiency\")\n",
    "    \n",
    "    # Add \"Slope Arrow\" to show direction\n",
    "    if preds_flat[-1] > preds_flat[0]:\n",
    "        color = 'green'\n",
    "        msg = \"Positive Impact\"\n",
    "    else:\n",
    "        color = 'orange'\n",
    "        msg = \"Negative Impact\"\n",
    "        \n",
    "    dx = max_val - min_val\n",
    "    dy = preds_flat[-1] - preds_flat[0]\n",
    "    # Only draw arrow if there is a significant change\n",
    "    if abs(dy) > 0.05:\n",
    "        plt.arrow(min_val, preds_flat[0], dx, dy, color=color, alpha=0.5, width=0.005)\n",
    "        plt.text(min_val, preds_flat[-1], msg, color=color, fontweight='bold')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure File Paths and Solar Parameters\n",
    "\n",
    "Sets up the project directory structure and solar panel configuration.\n",
    "\n",
    "**Same as ANN/KAN notebooks:**\n",
    "- 19 solar panels (8,360W total capacity)\n",
    "- Dual-orientation setup for all-day generation\n",
    "- Location: Bettystown, Ireland\n",
    "\n",
    "Refer to the ANN notebook for detailed parameter explanations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181a7834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the current path of the notebook\n",
    "notebook_path = os.path.abspath(\"big_project.ipynb\")\n",
    "notebook_dir = os.path.dirname(notebook_path).replace('\\\\', '/')\n",
    "print(\"Current notebook directory:\", notebook_dir)\n",
    "HOME_DIR = f'{notebook_dir}'\n",
    "DATA_DIR = f'{HOME_DIR}/data/'\n",
    "MODEL_DIR = f'{HOME_DIR}/model/'\n",
    "print(\"Data directory set to:\", DATA_DIR)\n",
    "RAW_DATA_DIR = f'{DATA_DIR}/raw_data/'\n",
    "TRAIN_DATA_DIR = f'{DATA_DIR}/training_data/'\n",
    "SQL_DB_PATH = f'{DATA_DIR}/db_sqlite/'\n",
    "SQL_DB_FILE = f'{SQL_DB_PATH}/big_project_db.sqlite3'\n",
    "BACKUP_FILE_TYPE = 'feather'  # Options: 'csv', 'feather', 'parquet'\n",
    "\n",
    "# Meteostat setup\n",
    "METEOSTAT_CACHE_DIR = f'{DATA_DIR}/meteostat_cache/'\n",
    "SOLAR_SITE_POSITION = (53.6985, -6.2080)  # Bettystown, Ireland\n",
    "LATITUDE, LONGITUDE = SOLAR_SITE_POSITION\n",
    "WEATHER_START_DATE = datetime.datetime(2024, 1, 1)\n",
    "WEATHER_END_DATE = datetime.datetime.now()\n",
    "# Solar panel configuration \n",
    "# Determined this using gemini and google maps measurements\n",
    "ROOF_PANE_I_ANGLE = 30  # degrees\n",
    "ROOF_PANE_II_ANGLE = 30  # degrees\n",
    "ROOF_PANE_I_AZIMUTH = 65  # degrees ( East-South-East)\n",
    "ROOF_PANE_II_AZIMUTH = 245  # degrees ( West-South-West)\n",
    "ROOF_PANE_I_COUNT = 7\n",
    "ROOF_PANE_II_COUNT = 12\n",
    "SOLAR_PANEL_POWER_RATING_W = 440  # Watts per panel\n",
    "TOTAL_SOLAR_PANE_I_CAPACITY_W = ROOF_PANE_I_COUNT * SOLAR_PANEL_POWER_RATING_W\n",
    "TOTAL_SOLAR_PANE_II_CAPACITY_W = ROOF_PANE_II_COUNT * SOLAR_PANEL_POWER_RATING_W\n",
    "TOTAL_SOLAR_CAPACITY_W = TOTAL_SOLAR_PANE_I_CAPACITY_W + TOTAL_SOLAR_PANE_II_CAPACITY_W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Nighttime Threshold\n",
    "\n",
    "Filters out nighttime data where Clear Sky GHI ≤ 50 W/m².\n",
    "\n",
    "**Note**: Variable name has typo (`hourly_nighlty_threshold` vs `hourly_nightly_threshold`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fcdff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_nighlty_threshold = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Filter Solar Data\n",
    "\n",
    "Loads the hourly solar dataset and removes nighttime records.\n",
    "\n",
    "**Data Format**: Feather format (Apache Arrow) for efficient storage and fast loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27006e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_hourly = pd.read_feather(f\"{TRAIN_DATA_DIR}/hourly_solar_full_data.feather\")\n",
    "\n",
    "# Remove all rows where Clear sky GHI is less than or equal to 50\n",
    "df_merge_hourly = df_merge_hourly[df_merge_hourly['Clear sky GHI'] > hourly_nighlty_threshold]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Weather Condition Features\n",
    "\n",
    "Extracts one-hot encoded weather features:\n",
    "- **Level 1**: 15 specific weather conditions\n",
    "- **Level 2**: 5 grouped categories\n",
    "\n",
    "These binary features help FFKAN understand how different weather conditions affect solar output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "level1_features = [level for level in df_merge_hourly.columns.tolist() if level.startswith('level1_')]\n",
    "level2_features = [level for level in df_merge_hourly.columns.tolist() if level.startswith('level2_')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation Placeholder\n",
    "\n",
    "**This cell appears to be empty** - likely a placeholder for additional data preparation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fd55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "display(pd.DataFrame({\"Columns\": df_merge_hourly.columns, \"Data Types\": df_merge_hourly.dtypes}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Preparation Placeholder\n",
    "\n",
    "**This cell appears to be empty** - continuing data preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7243c486",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = []\n",
    "# Columns: ['index', 'DateTime', 'PV(W)', 'Temperature(C)', 'Humidity(%)', 'Sunshine Duration', 'Condition Code', 'Precipitation(mm)', 'Dew Point(C)', 'Wind Direction(deg)', 'Wind Speed(m/s)', 'Wind Gust(m/s)', 'Pressure(hPa)', 'Snow Depth(cm)', 'level1_clear', 'level1_cloudy', 'level1_fair', 'level1_fog', 'level1_freezing_rain', 'level1_heavy_rain', 'level1_heavy_rain_shower', 'level1_heavy_sleet', 'level1_light_rain', 'level1_overcast', 'level1_rain', 'level1_rain_shower', 'level1_sleet', 'level1_sleet_shower', 'level1_thunderstorm', 'level2_good_visibility', 'level2_moderate_visibility', 'level2_poor_visibility', 'level2_precipitation', 'level2_severe_weather', '# Observation period', 'TOA', 'Clear sky GHI', 'Clear sky BHI', 'Clear sky DHI', 'Clear sky BNI', 'GHI', 'BHI', 'DHI', 'BNI', 'Reliability,', 'Time', 'Date', 'POA_Pane_I(W/m^2)', 'POA_Pane_II(W/m^2)', 'POAC_Pane_I(W/m^2)', 'POAC_Pane_II(W/m^2)', 'Power_Pane_I(W)', 'Power_Pane_II(W)', 'Power_ClearSky_Pane_I(W)', 'Power_ClearSky_Pane_II(W)', 'Total_Power_Output(W)', 'Total_Power_ClearSky_Output(W)', 'WeekOfYear', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']\n",
    "feature_cols = []\n",
    "test_no=\"999\"\n",
    "# Define target column\n",
    "\n",
    "target_col = 'PV(W)'\n",
    "# Kan Prefers Clearsky_Index\n",
    "#target_col = 'Clearsky_Index'\n",
    "#target_col = 'PV(W)_error'\n",
    "#target_col = 'PV(W)_error_index'\n",
    "#\n",
    "test_name=f\"Optimal Features  No Level 2 and No Clearsky - Target {target_col}\"\n",
    "notes=\"This is the best combination of features exclude level 2 and no clearsky weather features\"\n",
    "\n",
    "# Put change here to add more features\n",
    "feature_cols.append('Temperature(C)')\n",
    "feature_cols.append('Humidity(%)')\n",
    "feature_cols.append('Sunshine Duration')\n",
    "#feature_cols.append('Condition Code')\n",
    "feature_cols.append('Precipitation(mm)')\n",
    "feature_cols.append('Dew Point(C)')\n",
    "feature_cols.append('Wind Direction(deg)')\n",
    "feature_cols.append('Wind Speed(m/s)')\n",
    "feature_cols.append('Wind Gust(m/s)')\n",
    "feature_cols.append('Pressure(hPa)')\n",
    "#feature_cols.append('Snow Depth(cm)')\n",
    "feature_cols.append('Wind Cooling')\n",
    "#  level1_features\n",
    "#feature_cols.append('# Observation period')\n",
    "#feature_cols.append('TOA')\n",
    "#feature_cols.append('Clear sky GHI')\n",
    "#feature_cols.append('Clear sky BHI')\n",
    "#feature_cols.append('Clear sky DHI')\n",
    "#feature_cols.append('Clear sky BNI')\n",
    "# Relate to target #feature_cols.append('GHI')\n",
    "# Relate to target #feature_cols.append('BHI')\n",
    "# Relate to target #feature_cols.append('DHI')\n",
    "# Relate to target #feature_cols.append('BNI')\n",
    "# String ignore feature_cols.append('Reliability,')\n",
    "# Relate to target #feature_cols.append('POA_Pane_I(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POA_Pane_II(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POAC_Pane_I(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POAC_Pane_II(W/m^2)')\n",
    "# Relate to target #feature_cols.append('Power_Pane_I(W)')\n",
    "# Relate to target #feature_cols.append('Power_Pane_II(W)')\n",
    "feature_cols.append('Power_ClearSky_Pane_I(W)')\n",
    "feature_cols.append('Power_ClearSky_Pane_II(W)')\n",
    "# Relate to target #feature_cols.append('Total_Power_Output(W)')\n",
    "feature_cols.append('Total_Power_ClearSky_Output(W)')\n",
    "#feature_cols.append('WeekOfYear')\n",
    "feature_cols.append('Month_Sin')\n",
    "feature_cols.append('DayOfYear_Sin')\n",
    "feature_cols.append('HourOfDay_Sin')\n",
    "feature_cols.append('Month_Cos')\n",
    "feature_cols.append('DayOfYear_Cos')\n",
    "feature_cols.append('HourOfDay_Cos')\n",
    "#  level2_features\n",
    "feature_cols += level2_features\n",
    "#  level1_features\n",
    "#feature_cols += level1_features\n",
    "\n",
    "print(f\"\\nTesting Random Forest Regressor with target: {target_col} and features: {feature_cols}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further Preparation Placeholder\n",
    "\n",
    "**This cell appears to be empty** - continuing data preparation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fdc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Preparation Placeholder\n",
    "\n",
    "**This cell appears to be empty** - final data preparation steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9558cf61",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 1. Prepare Data (Using your winning \"Error\" target)\n",
    "# ==========================================\n",
    "\n",
    "# Assuming 'df' is your DataFrame and we want to predict 'PV_Error'\n",
    "# Define your features (X) and target (y)\n",
    "features = feature_cols\n",
    "target_col = target_col\n",
    "\n",
    "# Drop NaNs\n",
    "model_df = df_merge_hourly.dropna(subset=features + [target_col])\n",
    "X = model_df[features].values\n",
    "y = model_df[target_col].values.reshape(-1, 1)\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Normalize (Neural Networks LOVES scaled data)\n",
    "scaler_X = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "\n",
    "# Create dataset dictionary for pykan\n",
    "# pykan expects keys: 'train_input', 'train_label', 'test_input', 'test_label'\n",
    "dataset = {}\n",
    "dataset['train_input'] = torch.from_numpy(X_train_scaled).float()\n",
    "dataset['train_label'] = torch.from_numpy(y_train).float()\n",
    "dataset['test_input'] = torch.from_numpy(X_test_scaled).float()\n",
    "dataset['test_label'] = torch.from_numpy(y_test).float()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare FFKAN Model Input\n",
    "\n",
    "This cell likely:\n",
    "1. Selects features and target variable\n",
    "2. Splits data into train/test sets\n",
    "3. Standardizes features using StandardScaler\n",
    "4. Converts to PyTorch tensors\n",
    "5. Organizes into dataset dictionary\n",
    "\n",
    "**FFKAN Input Requirements**: Similar to KAN, requires properly formatted train/test tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3222d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Train FFKAN Model\n",
    "\n",
    "This cell:\n",
    "1. **Initializes** the FFKAN model with Fourier-based layers\n",
    "2. **Configures** Fourier frequency parameters\n",
    "3. **Trains** using standard PyTorch training loop\n",
    "\n",
    "**FFKAN Training Configuration:**\n",
    "- **Optimizer**: Adam with adaptive learning rates\n",
    "- **Loss Function**: Mean Squared Error (MSE)\n",
    "- **Batch Processing**: Mini-batch gradient descent\n",
    "- **Epochs**: Multiple passes through training data\n",
    "\n",
    "**Fourier Parameters:**\n",
    "- **Number of frequencies**: Controls model capacity\n",
    "- **Frequency range**: Determines which periodicities can be captured\n",
    "\n",
    "**Key Difference from Standard ANN:**\n",
    "Instead of learning just weights, FFKAN learns:\n",
    "- Fourier coefficients (amplitudes of sine/cosine terms)\n",
    "- Optimal frequency combinations\n",
    "- Non-linear transformations via Fourier series\n",
    "\n",
    "**Training Process:**\n",
    "Each iteration:\n",
    "1. Forward pass: Compute predictions using Fourier transformations\n",
    "2. Calculate loss: Compare predictions to actual values\n",
    "3. Backward pass: Compute gradients\n",
    "4. Update parameters: Adjust Fourier coefficients\n",
    "\n",
    "**References:**\n",
    "- [Neural Networks with Fourier Features](https://arxiv.org/abs/2006.10739)\n",
    "- [Adam Optimizer](https://arxiv.org/abs/1412.6980)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81463622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. Define & Train the FourierKAN\n",
    "# ==========================================\n",
    "\n",
    "# FourierKAN Model Wrapper to match pykan API\n",
    "class FourierKAN(torch.nn.Module):\n",
    "    def __init__(self, width, gridsize=300, device='cpu'):\n",
    "        super(FourierKAN, self).__init__()\n",
    "        self.width = width\n",
    "        self.gridsize = gridsize\n",
    "        self.device = device\n",
    "        \n",
    "        # Create layers based on width\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(width) - 1):\n",
    "            self.layers.append(\n",
    "                NaiveFourierKANLayer(\n",
    "                    inputdim=width[i],\n",
    "                    outdim=width[i+1],\n",
    "                    gridsize=gridsize,\n",
    "                    addbias=True,\n",
    "                    smooth_initialization=True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "    \n",
    "    def fit(self, dataset, opt='Adam', steps=100, lr=0.001, lamb=0.0, lamb_entropy=0.0):\n",
    "        \"\"\"\n",
    "        Train the FourierKAN model\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dict with keys train_input, train_label, test_input, test_label\n",
    "            opt: Optimizer name (Adam or LBFGS)\n",
    "            steps: Number of training steps\n",
    "            lr: Learning rate\n",
    "            lamb: L2 regularization (not used for now)\n",
    "            lamb_entropy: Entropy regularization (not used for now)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with train_loss and test_loss lists\n",
    "        \"\"\"\n",
    "        train_input = dataset['train_input'].to(self.device)\n",
    "        train_label = dataset['train_label'].to(self.device)\n",
    "        test_input = dataset['test_input'].to(self.device)\n",
    "        test_label = dataset['test_label'].to(self.device)\n",
    "        \n",
    "        # Choose optimizer\n",
    "        if opt == 'LBFGS':\n",
    "            optimizer = torch.optim.LBFGS(self.parameters(), lr=lr, max_iter=20)\n",
    "        else:\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "        \n",
    "        criterion = torch.nn.MSELoss()\n",
    "        \n",
    "        train_losses = []\n",
    "        test_losses = []\n",
    "        \n",
    "        for step in range(steps):\n",
    "            if opt == 'LBFGS':\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    output = self.forward(train_input)\n",
    "                    loss = criterion(output, train_label)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                \n",
    "                optimizer.step(closure)\n",
    "                \n",
    "                # Calculate losses\n",
    "                with torch.no_grad():\n",
    "                    train_output = self.forward(train_input)\n",
    "                    train_loss = criterion(train_output, train_label).item()\n",
    "                    test_output = self.forward(test_input)\n",
    "                    test_loss = criterion(test_output, test_label).item()\n",
    "            else:\n",
    "                optimizer.zero_grad()\n",
    "                output = self.forward(train_input)\n",
    "                loss = criterion(output, train_label)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_loss = loss.item()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    test_output = self.forward(test_input)\n",
    "                    test_loss = criterion(test_output, test_label).item()\n",
    "            \n",
    "            train_losses.append(train_loss)\n",
    "            test_losses.append(test_loss)\n",
    "            \n",
    "            if (step + 1) % 10 == 0:\n",
    "                print(f\"Step {step+1}/{steps}, Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}\")\n",
    "        \n",
    "        return {'train_loss': train_losses, 'test_loss': test_losses}\n",
    "    \n",
    "    def plot(self, beta=10, scale=1.2, in_vars=None, out_vars=None):\n",
    "        \"\"\"\n",
    "        Placeholder for plot function - FourierKAN doesn't have built-in visualization like pykan\n",
    "        \"\"\"\n",
    "        print(\"FourierKAN does not support the plot() method like pykan.\")\n",
    "        print(\"Use the plot_kan_feature() function to visualize individual features.\")\n",
    "    \n",
    "    def saveckpt(self, path):\n",
    "        \"\"\"\n",
    "        Save model checkpoint\n",
    "        \"\"\"\n",
    "        torch.save({\n",
    "            'model_state_dict': self.state_dict(),\n",
    "            'width': self.width,\n",
    "            'gridsize': self.gridsize,\n",
    "        }, path)\n",
    "        print(f\"Model saved to {path}\")\n",
    "    \n",
    "    def loadckpt(self, path):\n",
    "        \"\"\"\n",
    "        Load model checkpoint\n",
    "        \"\"\"\n",
    "        checkpoint = torch.load(path)\n",
    "        self.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Model loaded from {path}\")\n",
    "\n",
    "# Initialize FourierKAN\n",
    "input_dim = X_train.shape[1]\n",
    "print(f\"Input dimension for FourierKAN: {input_dim}\")\n",
    "\n",
    "model = FourierKAN(\n",
    "    width=[input_dim, 8, 1],\n",
    "    gridsize=300,  # Number of Fourier coefficients\n",
    "    device=device\n",
    ")\n",
    "\n",
    "print(\"Starting FourierKAN Training (using LBFGS optimizer)...\")\n",
    "print(f\"Input dimension: {input_dim}\")\n",
    "print(f\"Training samples: {X_train_scaled.shape[0]}\")\n",
    "print(f\"Test samples: {X_test_scaled.shape[0]}\")\n",
    "\n",
    "# Train the FourierKAN model using the dataset dictionary\n",
    "print(\"\\nTraining FourierKAN model with LBFGS optimizer...\")\n",
    "results = model.fit(\n",
    "    dataset,\n",
    "    opt='LBFGS',\n",
    "    steps=40,\n",
    "    lr=0.1,  # LBFGS typically uses higher learning rates\n",
    "    lamb=0.005,\n",
    "    lamb_entropy=0.01\n",
    ")\n",
    "\n",
    "print(\"\\n✓ FourierKAN Training Complete!\")\n",
    "print(f\"Training iterations completed: 40\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress\n",
    "\n",
    "**Note**: Unlike standard KAN, FFKAN typically doesn't have built-in visualization methods.\n",
    "\n",
    "This cell may:\n",
    "- Plot training/validation loss curves\n",
    "- Show convergence behavior\n",
    "- Display sample predictions\n",
    "\n",
    "**Why Different?**\n",
    "FFKAN is a newer/experimental architecture, so visualization tools are still being developed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc991c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FourierKAN does not support built-in visualization like pykan\n",
    "# We will use the plot_kan_feature function instead to visualize individual features\n",
    "print(\"Skipping model.plot() - FourierKAN uses Fourier coefficients, not splines\")\n",
    "print(\"See the feature plots below for visualization of learned functions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model Convergence\n",
    "\n",
    "Checks if the FFKAN model has converged:\n",
    "- Training loss stabilized\n",
    "- No signs of overfitting\n",
    "- Model ready for evaluation\n",
    "\n",
    "**Convergence Indicators:**\n",
    "- Loss decreases smoothly\n",
    "- Validation loss doesn't increase\n",
    "- Gradients are stable\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check for Convergence\n",
    "plt.plot(results['train_loss'], label='Train')\n",
    "plt.plot(results['test_loss'], label='Test')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fourier Activation Function Analysis\n",
    "\n",
    "**Note**: FFKAN doesn't expose individual activation functions the same way as KAN.\n",
    "\n",
    "**Why?**\n",
    "- Traditional KAN: Explicit spline functions per connection\n",
    "- FFKAN: Implicit functions via Fourier series superposition\n",
    "\n",
    "The activation behavior emerges from the combination of Fourier terms rather than explicit functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e939c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FourierKAN does not expose individual activation functions like pykan\n",
    "# The functions are represented as Fourier coefficients\n",
    "print(\"FourierKAN uses Fourier coefficients - no direct function extraction available\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bde1e5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f045b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in feature_cols:\n",
    "    plot_kan_feature(model, feature, feature_cols, X_train, scaler_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Individual Feature Effects\n",
    "\n",
    "Creates plots showing how each feature affects the output through the learned Fourier transformations.\n",
    "\n",
    "**Interpretation:**\n",
    "- Smooth curves indicate Fourier series approximation\n",
    "- Periodic patterns show captured daily/seasonal cycles\n",
    "- Amplitude indicates feature importance\n",
    "\n",
    "**Physical Relationships to Look For:**\n",
    "- **Temperature**: Inverse relationship at high temps (efficiency loss)\n",
    "- **Clear Sky GHI**: Strong positive correlation\n",
    "- **Hour of day**: Parabolic pattern (sunrise-noon-sunset)\n",
    "- **Humidity**: Likely negative relationship\n",
    "\n",
    "These plots validate if FFKAN learned physically reasonable relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# =========================================================\n",
    "# Helper Functions for Metrics Calculation\n",
    "# =========================================================\n",
    "\n",
    "def calculate_metrics_for_target(y_true, y_pred, target_name=\"Target\"):\n",
    "    \"\"\"Calculate comprehensive metrics for predictions\"\"\"\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Normalized metrics\n",
    "    capacity = y_true.max()\n",
    "    if capacity == 0:\n",
    "        n_mae = 0\n",
    "        n_rmse = 0\n",
    "    else:\n",
    "        n_mae = mae / capacity\n",
    "        n_rmse = rmse / capacity\n",
    "    accuracy = 1 - n_mae  # Simplified accuracy metric\n",
    "    \n",
    "    metrics = {\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'n_rmse': n_rmse,\n",
    "        'n_mae': n_mae,\n",
    "        'n_rmse_pct': n_rmse * 100,\n",
    "        'n_mae_pct': n_mae * 100,\n",
    "        'accuracy': accuracy\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{target_name} Metrics:\")\n",
    "    print(f\"  RMSE:    {rmse:.2f}\")\n",
    "    print(f\"  MAE:     {mae:.2f}\")\n",
    "    print(f\"  R²:      {r2:.4f}\")\n",
    "    print(f\"  N-RMSE:  {n_rmse:.4f} ({n_rmse*100:.2f}%)\")\n",
    "    print(f\"  N-MAE:   {n_mae:.4f} ({n_mae*100:.2f}%)\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def convert_to_pvw(y_pred, y_true, target_col, clearsky_values):\n",
    "    \"\"\"Convert predictions to PV(W) based on target column type\"\"\"\n",
    "    if target_col == 'PV(W)':\n",
    "        # Already in PV(W), no conversion needed\n",
    "        return y_pred.copy(), y_true.copy()\n",
    "    elif target_col == 'Clearsky_Index':\n",
    "        # PV(W) = Clearsky_Index * Total_Power_ClearSky_Output(W)\n",
    "        y_pred_pvw = y_pred * clearsky_values\n",
    "        y_true_pvw = y_true * clearsky_values\n",
    "    elif target_col == 'PV(W)_error':\n",
    "        # PV(W) = PV(W)_error + Total_Power_ClearSky_Output(W)\n",
    "        y_pred_pvw = y_pred + clearsky_values\n",
    "        y_true_pvw = y_true + clearsky_values\n",
    "    elif target_col == 'PV(W)_error_index':\n",
    "        # PV(W) = PV(W)_error_index * Total_Power_ClearSky_Output(W) + Total_Power_ClearSky_Output(W)\n",
    "        y_pred_pvw = y_pred * clearsky_values + clearsky_values\n",
    "        y_true_pvw = y_true * clearsky_values + clearsky_values\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown target column: {target_col}\")\n",
    "    \n",
    "    # Apply physics constraints (no negative power output)\n",
    "    y_pred_pvw = np.clip(y_pred_pvw, 0, None)\n",
    "    y_true_pvw = np.clip(y_true_pvw, 0, None)\n",
    "    \n",
    "    return y_pred_pvw, y_true_pvw\n",
    "\n",
    "# =========================================================\n",
    "# Get Predictions from the Model\n",
    "# =========================================================\n",
    "\n",
    "# Get predictions on test set\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_input = dataset['test_input'].to(model.device)\n",
    "    train_input = dataset['train_input'].to(model.device)\n",
    "    y_pred_test_tensor = model(test_input)\n",
    "    y_pred_train_tensor = model(train_input)\n",
    "\n",
    "y_pred_test = y_pred_test_tensor.cpu().numpy()\n",
    "y_pred_train = y_pred_train_tensor.cpu().numpy()\n",
    "\n",
    "print(f\"Predictions generated: test={y_pred_test.shape}, train={y_pred_train.shape}\")\n",
    "\n",
    "# =========================================================\n",
    "# Extract Clearsky Values from Features\n",
    "# =========================================================\n",
    "# Find the index of 'Total_Power_ClearSky_Output(W)' in feature_cols\n",
    "clearsky_index = feature_cols.index('Total_Power_ClearSky_Output(W)')\n",
    "\n",
    "# Extract clearsky values from scaled features and inverse transform\n",
    "clearsky_test_scaled = X_test_scaled[:, clearsky_index].reshape(-1, 1)\n",
    "clearsky_train_scaled = X_train_scaled[:, clearsky_index].reshape(-1, 1)\n",
    "\n",
    "# Create dummy arrays for inverse transform\n",
    "dummy_test = np.zeros((X_test_scaled.shape[0], X_train.shape[1]))\n",
    "dummy_test[:, clearsky_index] = clearsky_test_scaled.flatten()\n",
    "clearsky_test = scaler_X.inverse_transform(dummy_test)[:, clearsky_index].reshape(-1, 1)\n",
    "\n",
    "dummy_train = np.zeros((X_train_scaled.shape[0], X_train.shape[1]))\n",
    "dummy_train[:, clearsky_index] = clearsky_train_scaled.flatten()\n",
    "clearsky_train = scaler_X.inverse_transform(dummy_train)[:, clearsky_index].reshape(-1, 1)\n",
    "\n",
    "print(f\"Clearsky values extracted: test={clearsky_test.shape}, train={clearsky_train.shape}\")\n",
    "\n",
    "# =========================================================\n",
    "# Calculate Metrics for Configured Target Column\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Metrics for Configured Target: {target_col}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Apply physics constraints to predictions\n",
    "y_pred_test_clipped = np.clip(y_pred_test, 0, None)\n",
    "y_pred_train_clipped = np.clip(y_pred_train, 0, None)\n",
    "\n",
    "test_metrics = calculate_metrics_for_target(y_test, y_pred_test_clipped, f\"Test - {target_col}\")\n",
    "train_metrics = calculate_metrics_for_target(y_train, y_pred_train_clipped, f\"Train - {target_col}\")\n",
    "\n",
    "# =========================================================\n",
    "# Convert to PV(W) and Calculate PV(W) Metrics\n",
    "# =========================================================\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Converting to PV(W) and Calculating PV(W) Metrics\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Convert predictions and actual values to PV(W)\n",
    "y_pred_test_pvw, y_test_pvw = convert_to_pvw(y_pred_test, y_test, target_col, clearsky_test)\n",
    "y_pred_train_pvw, y_train_pvw = convert_to_pvw(y_pred_train, y_train, target_col, clearsky_train)\n",
    "\n",
    "# Calculate PV(W) metrics\n",
    "test_metrics_pvw = calculate_metrics_for_target(y_test_pvw, y_pred_test_pvw, \"Test - PV(W)\")\n",
    "train_metrics_pvw = calculate_metrics_for_target(y_train_pvw, y_pred_train_pvw, \"Train - PV(W)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Metrics Calculation Complete\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Performance Metrics\n",
    "\n",
    "Evaluates FFKAN model using comprehensive regression metrics:\n",
    "\n",
    "**Metrics:**\n",
    "1. **RMSE**: Root Mean Squared Error (penalizes large errors)\n",
    "2. **MAE**: Mean Absolute Error (average error magnitude)\n",
    "3. **R²**: Coefficient of determination (variance explained)\n",
    "4. **N-RMSE**: Normalized RMSE (as % of range)\n",
    "5. **N-MAE**: Normalized MAE (as % of range)\n",
    "6. **Accuracy**: 1 - N-MAE (simplified accuracy measure)\n",
    "\n",
    "**Two-Stage Evaluation:**\n",
    "1. On configured target (e.g., Clearsky_Index)\n",
    "2. Converted to PV(W) for real-world interpretation\n",
    "\n",
    "**Model Comparison:**\n",
    "These metrics enable direct comparison with:\n",
    "- Standard ANN (feedforward neural network)\n",
    "- KAN (B-spline based)\n",
    "- FFKAN (Fourier based)\n",
    "\n",
    "**Reference:**\n",
    "- [Regression Model Evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb02911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================\n",
    "# Save Results to CSV\n",
    "# =========================================================\n",
    "import os\n",
    "\n",
    "# Define test parameters\n",
    "test_no = 1  # Increment this for each test run\n",
    "test_name = f\"Optimal Features  No Level 2 and No Clearsky - Target {target_col}\"\n",
    "notes = \"\"  # Add any notes about this test run\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir = 'results'\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "\n",
    "# Model name\n",
    "model_name = \"FFKAN\"\n",
    "\n",
    "# CSV file path\n",
    "csv_file = f'{results_dir}/{model_name.lower()}_test_metrics.csv'\n",
    "\n",
    "# Prepare CSV line\n",
    "with open(csv_file, 'a') as f:\n",
    "    # If file is empty, write header\n",
    "    if not os.path.exists(csv_file) or os.path.getsize(csv_file) == 0:\n",
    "        header = ['Model', 'Test No', 'Target Column',\n",
    "                  'Test Name', 'Test RMSE', 'Test MAE', 'Test R2', 'Test N-RMSE', 'Test N-MAE','Test N-RMSE %', 'Test N-MAE %', 'Test Accuracy',\n",
    "                  'Train RMSE', 'Train MAE', 'Train R2', 'Train N-RMSE', 'Train N-MAE', 'Train N-RMSE %', 'Train N-MAE %', 'Train Accuracy', \n",
    "                  'Test PV(W) RMSE', 'Test PV(W) MAE', 'Test PV(W) R2', 'Test PV(W) N-RMSE', 'Test PV(W) N-MAE','Test PV(W) N-RMSE %', 'Test PV(W) N-MAE %', 'Test PV(W) Accuracy',\n",
    "                  'Train PV(W) RMSE', 'Train PV(W) MAE', 'Train PV(W) R2', 'Train PV(W) N-RMSE', 'Train PV(W) N-MAE', 'Train PV(W) Accuracy','Train PV(W) N-RMSE %', 'Train PV(W) N-MAE %',\n",
    "                  'Notes', 'Feature Columns']\n",
    "        f.write(','.join(header) + '\\\\n')\n",
    "    \n",
    "    line = []\n",
    "    line.append(model_name)\n",
    "    line.append(str(test_no))\n",
    "    line.append(f\"{target_col}\")\n",
    "    line.append(f'\"{test_name}\"')  # Quote in case of commas\n",
    "    line.append(f\"{test_metrics['rmse']:.2f}\")\n",
    "    line.append(f\"{test_metrics['mae']:.2f}\")\n",
    "    line.append(f\"{test_metrics['r2']:.4f}\")\n",
    "    line.append(f\"{test_metrics['n_rmse']:.4f}\")\n",
    "    line.append(f\"{test_metrics['n_mae']:.4f}\")\n",
    "    line.append(f\"{test_metrics['n_rmse_pct']:.4f}\")\n",
    "    line.append(f\"{test_metrics['n_mae_pct']:.4f}\")\n",
    "    line.append(f\"{test_metrics['accuracy']:.4f}\")\n",
    "    line.append(f\"{train_metrics['rmse']:.2f}\")\n",
    "    line.append(f\"{train_metrics['mae']:.2f}\")\n",
    "    line.append(f\"{train_metrics['r2']:.4f}\")\n",
    "    line.append(f\"{train_metrics['n_rmse']:.4f}\")\n",
    "    line.append(f\"{train_metrics['n_mae']:.4f}\")\n",
    "    line.append(f\"{train_metrics['n_rmse_pct']:.4f}\")\n",
    "    line.append(f\"{train_metrics['n_mae_pct']:.4f}\")\n",
    "    line.append(f\"{train_metrics['accuracy']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['rmse']:.2f}\")\n",
    "    line.append(f\"{test_metrics_pvw['mae']:.2f}\")\n",
    "    line.append(f\"{test_metrics_pvw['r2']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['n_rmse']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['n_mae']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['n_rmse_pct']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['n_mae_pct']:.4f}\")\n",
    "    line.append(f\"{test_metrics_pvw['accuracy']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['rmse']:.2f}\")\n",
    "    line.append(f\"{train_metrics_pvw['mae']:.2f}\")\n",
    "    line.append(f\"{train_metrics_pvw['r2']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['n_rmse']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['n_mae']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['accuracy']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['n_rmse_pct']:.4f}\")\n",
    "    line.append(f\"{train_metrics_pvw['n_mae_pct']:.4f}\")\n",
    "    line.append(f'\"{notes}\"')\n",
    "    line.append(f'\"feature_cols: {\":\".join(feature_cols)}\"')\n",
    "    f.write(','.join(line) + '\\\\n')\n",
    "\n",
    "print(f\"\\\\nResults saved to {csv_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f6c1df",
   "metadata": {},
   "source": [
    "__Save Model__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec891fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.saveckpt(f\"{MODEL_DIR}/kan_model_target.ckpt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
