{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "# xlrd is required for reading xls Excel files\n",
    "import xlrd\n",
    "import re\n",
    "import sqlite3\n",
    "import meteostat as mt\n",
    "# Use this when displaying markdown in Jupyter Notebooks ( Gemini suggestion )\n",
    "from IPython.display import display, Markdown\n",
    "# do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d82e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2ae666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the current path of the notebook\n",
    "notebook_path = os.path.abspath(\"big_project.ipynb\")\n",
    "notebook_dir = os.path.dirname(notebook_path).replace('\\\\', '/')\n",
    "print(\"Current notebook directory:\", notebook_dir)\n",
    "HOME_DIR = f'{notebook_dir}'\n",
    "DATA_DIR = f'{HOME_DIR}/data/'\n",
    "print(\"Data directory set to:\", DATA_DIR)\n",
    "RAW_DATA_DIR = f'{DATA_DIR}/raw_data/'\n",
    "TRAIN_DATA_DIR = f'{DATA_DIR}/training_data/'\n",
    "SQL_DB_PATH = f'{DATA_DIR}/db_sqlite/'\n",
    "SQL_DB_FILE = f'{SQL_DB_PATH}/big_project_db.sqlite3'\n",
    "BACKUP_FILE_TYPE = 'feather'  # Options: 'csv', 'feather', 'parquet'\n",
    "# Plotly setup\n",
    "plt.style.use('classic')\n",
    "sns.set_style('whitegrid')\n",
    "# Meteostat setup\n",
    "METEOSTAT_CACHE_DIR = f'{DATA_DIR}/meteostat_cache/'\n",
    "SOLAR_SITE_POSITION = (53.6985, -6.2080)  # Bettystown, Ireland\n",
    "LATITUDE, LONGITUDE = SOLAR_SITE_POSITION\n",
    "WEATHER_START_DATE = datetime.datetime(2024, 1, 1)\n",
    "WEATHER_END_DATE = datetime.datetime.now()\n",
    "# Solar panel configuration \n",
    "# Determined this using gemini and google maps measurements\n",
    "ROOF_PANE_I_ANGLE = 30  # degrees\n",
    "ROOF_PANE_II_ANGLE = 30  # degrees\n",
    "ROOF_PANE_I_AZIMUTH = 65  # degrees ( East-South-East)\n",
    "ROOF_PANE_II_AZIMUTH = 245  # degrees ( West-South-West)\n",
    "ROOF_PANE_I_COUNT = 7\n",
    "ROOF_PANE_II_COUNT = 12\n",
    "SOLAR_PANEL_POWER_RATING_W = 440  # Watts per panel\n",
    "TOTAL_SOLAR_PANE_I_CAPACITY_W = ROOF_PANE_I_COUNT * SOLAR_PANEL_POWER_RATING_W\n",
    "TOTAL_SOLAR_PANE_II_CAPACITY_W = ROOF_PANE_II_COUNT * SOLAR_PANEL_POWER_RATING_W\n",
    "TOTAL_SOLAR_CAPACITY_W = TOTAL_SOLAR_PANE_I_CAPACITY_W + TOTAL_SOLAR_PANE_II_CAPACITY_W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_nighlty_threshold = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Regression Analysis Of Solar and Weather\n\n[XBOOST](https://xgboost.readthedocs.io/en/stable/)"
   ],
   "id": "864beae2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315ae47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "except:\n",
    "    %pip install xgboost\n",
    "    import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34515210",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_enriched_features = \"data/processed_data/hourly_solar_copernicus_enriched_data.feather\"\n",
    "file_weather_data = \"data/processed_data/hourly_weather_data.feather\"\n",
    "file_solar_data=\"data/processed_data/daily_solar_data.feather\"\n",
    "file_training_data = f\"{TRAIN_DATA_DIR}/hourly_solar_training_data.feather\"\n",
    "file_testing_data = f\"{TRAIN_DATA_DIR}/hourly_solar_testing_data.feather\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79261753",
   "metadata": {},
   "source": [
    "__Load Test and Training Data_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc9febc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_daytime_train = pd.read_feather(f\"{TRAIN_DATA_DIR}/hourly_solar_training_data.feather\")\n",
    "df_daytime_test = pd.read_feather(f\"{TRAIN_DATA_DIR}/hourly_solar_testing_data.feather\")\n",
    "\n",
    "# use list comprehension to get list of columns for level 1 and level 2 from weather data\n",
    "# level 1 and levl 2 are based on condition codes from meteostat - it onehot encoding of weather conditions , with level 1 lowest level , and level 2 a summary level\n",
    "level1_features = [level for level in df_daytime_train.columns.tolist() if level.startswith('level1_')]\n",
    "level2_features = [level for level in df_daytime_train.columns.tolist() if level.startswith('level2_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c61aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column list and data types and index\n",
    "display(pd.DataFrame({\"Columns\": df_daytime_train.columns, \"Data Types\": df_daytime_train.dtypes}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c96674",
   "metadata": {},
   "source": [
    "## Histogram of various solar measures \n",
    "\n",
    "This shows a histogram of the various measures solar output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5d208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "feature_cols = []\n",
    "# Columns: ['index', 'DateTime', 'PV(W)', 'Temperature(C)', 'Humidity(%)', 'Sunshine Duration', 'Condition Code', 'Precipitation(mm)', 'Dew Point(C)', 'Wind Direction(deg)', 'Wind Speed(m/s)', 'Wind Gust(m/s)', 'Pressure(hPa)', 'Snow Depth(cm)', 'level1_clear', 'level1_cloudy', 'level1_fair', 'level1_fog', 'level1_freezing_rain', 'level1_heavy_rain', 'level1_heavy_rain_shower', 'level1_heavy_sleet', 'level1_light_rain', 'level1_overcast', 'level1_rain', 'level1_rain_shower', 'level1_sleet', 'level1_sleet_shower', 'level1_thunderstorm', 'level2_good_visibility', 'level2_moderate_visibility', 'level2_poor_visibility', 'level2_precipitation', 'level2_severe_weather', '# Observation period', 'TOA', 'Clear sky GHI', 'Clear sky BHI', 'Clear sky DHI', 'Clear sky BNI', 'GHI', 'BHI', 'DHI', 'BNI', 'Reliability,', 'Time', 'Date', 'POA_Pane_I(W/m^2)', 'POA_Pane_II(W/m^2)', 'POAC_Pane_I(W/m^2)', 'POAC_Pane_II(W/m^2)', 'Power_Pane_I(W)', 'Power_Pane_II(W)', 'Power_ClearSky_Pane_I(W)', 'Power_ClearSky_Pane_II(W)', 'Total_Power_Output(W)', 'Total_Power_ClearSky_Output(W)', 'WeekOfYear', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']\n",
    "feature_cols = []\n",
    "test_no=\"999\"\n",
    "# Define target column\n",
    "\n",
    "#target_col = 'PV(W)'\n",
    "#target_col = 'Clearsky_Index'\n",
    "target_col = 'PV(W)_error'\n",
    "#target_col = 'PV(W)_error_index'\n",
    "#\n",
    "test_name=f\"Optimal Features  No Level 2 and No Clearsky - Target {target_col}\"\n",
    "notes=\"This is the best combination of features exclude level 2 and no clearsky weather features\"\n",
    "\n",
    "# Put change here to add more features\n",
    "feature_cols.append('Temperature(C)')\n",
    "feature_cols.append('Humidity(%)')\n",
    "feature_cols.append('Sunshine Duration')\n",
    "#feature_cols.append('Condition Code')\n",
    "feature_cols.append('Precipitation(mm)')\n",
    "feature_cols.append('Dew Point(C)')\n",
    "feature_cols.append('Wind Direction(deg)')\n",
    "feature_cols.append('Wind Speed(m/s)')\n",
    "#feature_cols.append('Wind Gust(m/s)')\n",
    "feature_cols.append('Pressure(hPa)')\n",
    "#feature_cols.append('Snow Depth(cm)')\n",
    "feature_cols.append('Wind Cooling')\n",
    "#  level1_features\n",
    "#feature_cols.append('# Observation period')\n",
    "#feature_cols.append('TOA')\n",
    "#feature_cols.append('Clear sky GHI')\n",
    "#feature_cols.append('Clear sky BHI')\n",
    "#feature_cols.append('Clear sky DHI')\n",
    "#feature_cols.append('Clear sky BNI')\n",
    "# Relate to target #feature_cols.append('GHI')\n",
    "# Relate to target #feature_cols.append('BHI')\n",
    "# Relate to target #feature_cols.append('DHI')\n",
    "# Relate to target #feature_cols.append('BNI')\n",
    "# String ignore feature_cols.append('Reliability,')\n",
    "# Relate to target #feature_cols.append('POA_Pane_I(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POA_Pane_II(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POAC_Pane_I(W/m^2)')\n",
    "# Relate to target #feature_cols.append('POAC_Pane_II(W/m^2)')\n",
    "# Relate to target #feature_cols.append('Power_Pane_I(W)')\n",
    "# Relate to target #feature_cols.append('Power_Pane_II(W)')\n",
    "#feature_cols.append('Power_ClearSky_Pane_I(W)')\n",
    "#feature_cols.append('Power_ClearSky_Pane_II(W)')\n",
    "# Relate to target #feature_cols.append('Total_Power_Output(W)')\n",
    "feature_cols.append('Total_Power_ClearSky_Output(W)')\n",
    "#feature_cols.append('WeekOfYear')\n",
    "feature_cols.append('Month_Sin')\n",
    "feature_cols.append('DayOfYear_Sin')\n",
    "feature_cols.append('HourOfDay_Sin')\n",
    "#  level2_features\n",
    "feature_cols += level2_features\n",
    "#  level1_features\n",
    "#feature_cols += level1_features\n",
    "\n",
    "print(f\"\\nTesting XGBoost Regressor with target: {target_col} and features: {feature_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8477367a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(f\"\\nTesting XGBoost Regressor with target: {target_col} and features: {feature_cols}\")\n",
    "# Create x and y for training and testing\n",
    "X_train = df_daytime_train[feature_cols]\n",
    "y_train = df_daytime_train[target_col]\n",
    "X_test = df_daytime_test[feature_cols]\n",
    "y_test = df_daytime_test[target_col]\n",
    "# print shapes\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "# Save test and training x and y to feather files\n",
    "# This allows later analysis without needing to redo the train test split\n",
    "X_train.to_pickle(f\"{TRAIN_DATA_DIR}/X_train_test_no_{test_no}.pickle\")\n",
    "y_train.to_pickle(f\"{TRAIN_DATA_DIR}/y_train_test_no_{test_no}.pickle\")\n",
    "X_test.to_pickle(f\"{TRAIN_DATA_DIR}/X_test_test_no_{test_no}.pickle\")\n",
    "y_test.to_pickle(f\"{TRAIN_DATA_DIR}/y_test_test_no_{test_no}.pickle\")\n",
    "# Also put test no , name and notes in a dataframe and save as feather\n",
    "test_info_df = pd.DataFrame({\n",
    "    \"Test No\": [test_no],\n",
    "    \"Test Name\": [test_name],\n",
    "    \"Notes\": [notes],\n",
    "    'target_col': [target_col],\n",
    "    'feature_cols': [feature_cols]\n",
    "})\n",
    "test_info_df.to_feather(f\"{TRAIN_DATA_DIR}/test_info_test_no_{test_no}.pickle\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost Regressor with optimized hyperparameters\n# n_estimators: Number of boosting rounds (trees)\n# learning_rate: Step size shrinkage used to prevent overfitting\n# max_depth: Maximum depth of a tree\n# subsample: Fraction of samples used for fitting the individual base learners\n# colsample_bytree: Fraction of features used when constructing each tree\n# reg_alpha: L1 regularization term on weights\n# reg_lambda: L2 regularization term on weights\nxgb_model = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.1,\n    max_depth=10,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    n_jobs=-1\n)\n\n# Train the model\nxgb_model.fit(X_train, y_train)\n\n# Make predictions\ny_pred_train = xgb_model.predict(X_train)\ny_pred_test = xgb_model.predict(X_test)\n"
   ],
   "id": "xgb_model_train"
  },
  {
   "cell_type": "markdown",
   "id": "de79b9cd",
   "metadata": {},
   "source": [
    "__Metrics for Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 main metrics: RMSE, MAE, R2 for both training and testing\nfrom sklearn.metrics import mean_absolute_error\n\n# Calculate metrics for residuals (target_col)\ntrain_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\ntrain_mae = mean_absolute_error(y_train, y_pred_train)\ntrain_r2 = r2_score(y_train, y_pred_train)\ntrain_n_rmse = train_rmse / y_train.mean()\ntrain_n_mae = train_mae / y_train.mean()\ntrain_accuracy = 1 - train_n_rmse\n\ntest_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\ntest_mae = mean_absolute_error(y_test, y_pred_test)\ntest_r2 = r2_score(y_test, y_pred_test)\ntest_n_rmse = test_rmse / y_test.mean()\ntest_n_mae = test_mae / y_test.mean()\ntest_accuracy = 1 - test_n_rmse\n\nprint(f\"\\nXGBoost Model Results for {target_col}:\")\nprint(f\"Train RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, R2: {train_r2:.4f}, N-RMSE: {train_n_rmse:.4f}, N-MAE: {train_n_mae:.4f}, Accuracy: {train_accuracy:.4f}\")\nprint(f\"Test  RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, R2: {test_r2:.4f}, N-RMSE: {test_n_rmse:.4f}, N-MAE: {test_n_mae:.4f}, Accuracy: {test_accuracy:.4f}\")\n\n# Reconstruct actual PV(W) values for final evaluation\ny_train_actual = y_train + df_daytime_train['Total_Power_ClearSky_Output(W)']\ny_pred_train_actual = y_pred_train + df_daytime_train['Total_Power_ClearSky_Output(W)']\ny_test_actual = y_test + df_daytime_test['Total_Power_ClearSky_Output(W)']\ny_pred_test_actual = y_pred_test + df_daytime_test['Total_Power_ClearSky_Output(W)']\n\n# Clip negative predictions to 0\ny_pred_train_actual = y_pred_train_actual.clip(lower=0)\ny_pred_test_actual = y_pred_test_actual.clip(lower=0)\n\n# Calculate metrics for actual PV(W) predictions\ntrain_rmse_pvw = np.sqrt(mean_squared_error(y_train_actual, y_pred_train_actual))\ntrain_mae_pvw = mean_absolute_error(y_train_actual, y_pred_train_actual)\ntrain_r2_pvw = r2_score(y_train_actual, y_pred_train_actual)\ntrain_n_rmse_pvw = train_rmse_pvw / y_train_actual.mean()\ntrain_n_mae_pvw = train_mae_pvw / y_train_actual.mean()\ntrain_accuracy_pvw = 1 - train_n_rmse_pvw\n\ntest_rmse_pvw = np.sqrt(mean_squared_error(y_test_actual, y_pred_test_actual))\ntest_mae_pvw = mean_absolute_error(y_test_actual, y_pred_test_actual)\ntest_r2_pvw = r2_score(y_test_actual, y_pred_test_actual)\ntest_n_rmse_pvw = test_rmse_pvw / y_test_actual.mean()\ntest_n_mae_pvw = test_mae_pvw / y_test_actual.mean()\ntest_accuracy_pvw = 1 - test_n_rmse_pvw\n\nprint(f\"\\nXGBoost Model Results for PV(W):\")\nprint(f\"Train RMSE: {train_rmse_pvw:.2f}, MAE: {train_mae_pvw:.2f}, R2: {train_r2_pvw:.4f}, N-RMSE: {train_n_rmse_pvw:.4f}, N-MAE: {train_n_mae_pvw:.4f}, Accuracy: {train_accuracy_pvw:.4f}\")\nprint(f\"Test  RMSE: {test_rmse_pvw:.2f}, MAE: {test_mae_pvw:.2f}, R2: {test_r2_pvw:.4f}, N-RMSE: {test_n_rmse_pvw:.4f}, N-MAE: {test_n_mae_pvw:.4f}, Accuracy: {test_accuracy_pvw:.4f}\")\n"
   ],
   "id": "metrics_calc"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ""
   ],
   "id": "empty_md_1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature importance plot\nimportances = xgb_model.feature_importances_\nfeature_importance_df = pd.DataFrame({\n    'Feature': feature_cols,\n    'Importance': importances\n}).sort_values(by='Importance', ascending=False)\n\nprint(feature_importance_df.head(20))\n"
   ],
   "id": "feat_imp_df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\nplt.figure(figsize=(10, 6))\nplt.title(f'Feature Importance Target {target_col}')\nsns.barplot(x='Importance', y='Feature', data=feature_importance_df.head(20), palette='viridis')\nplt.xlabel('Importance')\nplt.ylabel('Feature')\nplt.tight_layout()\nplt.show()\n"
   ],
   "id": "feat_imp_plot"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "#import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(2,1,figsize=(10,6))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# use sns scatter plot to show residuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.scatterplot(x=y_test, y=y_test - y_test_pred, color='blue', label='Test Data', alpha=0.5 ,ax=ax[0])\n",
    "sns.scatterplot(x=y_train, y=y_train - y_train_pred, color='green', label='Train Data', alpha=0.4, ax=ax[0])\n",
    "ax[0].axhline(y=0, color='red', linestyle='--')\n",
    "ax[0].set_title(f'Residuals vs Actual {target_col}')\n",
    "ax[0].set_xlabel(f'Actual {target_col}')\n",
    "ax[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "ax[0].legend()\n",
    "\n",
    "# use sns scatter plot to show residuals\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.scatterplot(x=y_test_pvw, y=y_test_pvw - y_test_pvw_pred, color='blue', label='Test Data', alpha=0.5 ,ax=ax[1])\n",
    "sns.scatterplot(x=y_train_pvw, y=y_train_pvw - y_train_pvw_pred, color='green', label='Train Data', alpha=0.4, ax=ax[1])\n",
    "ax[1].axhline(y=0, color='red', linestyle='--')\n",
    "ax[1].set_title(f'Residuals vs Actual PV(W)')\n",
    "ax[1].set_xlabel(f'Actual PV(W)')\n",
    "ax[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot risduals per hour of day\n",
    "fig, ax = plt.subplots(2,1,figsize=(10,12))\n",
    "ax = ax.flatten()\n",
    "sns.scatterplot(x=df_daytime_test['DateTime'].dt.hour, y=y_test - y_test_pred, color='blue', label='Test Data', alpha=0.5,ax=ax[0])\n",
    "sns.scatterplot(x=df_daytime_train['DateTime'].dt.hour, y=y_train - y_train_pred\n",
    ", color='green', label='Train Data', alpha=0.4, ax=ax[0])\n",
    "ax[0].axhline(y=0, color='red', linestyle='--')\n",
    "ax[0].set_title('Residuals vs Hour of Day')\n",
    "ax[0].set_xlabel('Hour of Day')\n",
    "ax[0].set_ylabel('Residuals (Actual - Predicted)')\n",
    "ax[0].legend()\n",
    "sns.scatterplot(x=df_daytime_test['DateTime'].dt.hour, y=y_test_pvw - y_test_pvw_pred, color='blue', label='Test Data', alpha=0.5,ax=ax[1])\n",
    "sns.scatterplot(x=df_daytime_train['DateTime'].dt.hour, y=y_train_pvw - y_train_pvw_pred\n",
    ", color='green', label='Train Data', alpha=0.4, ax=ax[1])\n",
    "ax[1].axhline(y=0, color='red', linestyle='--')\n",
    "ax[1].set_title('Residuals vs Hour of Day')\n",
    "ax[1].set_xlabel('Hour of Day')\n",
    "ax[1].set_ylabel('Residuals (Actual - Predicted)')\n",
    "ax[1].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual histogram\n",
    "def plot_residuals_histogram():\n",
    "    # Keep the main fig and ax definition\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(18, 14)) # Slightly taller to prevent title overlap\n",
    "    ax = ax.flatten()\n",
    "    fig.suptitle('Residuals Analysis', fontsize=18)\n",
    "    \n",
    "    # Subplot 1: Histograms\n",
    "    sns.histplot(y_test - y_test_pred, color='blue', label='Test Data', kde=True, bins=30, ax=ax[0])\n",
    "    sns.histplot(y_train - y_train_pred, color='green', label='Train Data', kde=True, bins=30, ax=ax[0])\n",
    "    ax[0].set_title('Residuals Histogram')\n",
    "    ax[0].legend() # Added legend here so you can see which is which\n",
    "\n",
    "    # Subplot 2: Density (Removed the extra plt.figure call)\n",
    "    sns.kdeplot(y_test - y_test_pred, color='blue', label='Test Data', fill=True, ax=ax[1])\n",
    "    sns.kdeplot(y_train - y_train_pred, color='green', label='Train Data', fill=True, ax=ax[1])\n",
    "    ax[1].set_title('Residuals Density Plot')\n",
    "    ax[1].set_xlabel('Residuals (Actual - Predicted)')\n",
    "    ax[1].set_ylabel('Density')\n",
    "    ax[1].legend()\n",
    "    # plot difference of residuals between train and test as histogram\n",
    "    sns.histplot((y_test - y_test_pred) - (y_train - y_train_pred), color='red', label='Test - Train Residuals', kde=True, stat=\"count\", bins=30, ax=ax[2])\n",
    "    ax[2].set_title('Difference of Residuals Histogram')\n",
    "    ax[2].set_xlabel('Difference of Residuals')\n",
    "    ax[2].set_ylabel('Frequency')\n",
    "    ax[2].legend()\n",
    "\n",
    "    # plot difference of residuals\n",
    "    sns.kdeplot((y_test - y_test_pred) - (y_train - y_train_pred), color='red', label='Test - Train Residuals', fill=True, ax=ax[3])\n",
    "    ax[3].set_title('Difference of Residuals Density Plot')\n",
    "    ax[3].set_xlabel('Difference of Residuals')\n",
    "    ax[3].set_ylabel('Density')\n",
    "    ax[3].legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_residuals_histogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7d70d6",
   "metadata": {},
   "source": [
    "### **\ud83d\udd2c Deep Dive: Spatiotemporal Error Analysis**\n",
    "\n",
    "This section explores the specific conditions under which the Random Forest model underperforms. By isolating the residuals in the **95th percentile** (extreme errors) and mapping them against temporal features, we uncover the model's \"blind spots.\"\n",
    "\n",
    "#### **1\\. The \"Error Loop\" (Convex Hull Analysis)**\n",
    "\n",
    "Visualizing the high-error grouping area reveals a distinct geometric pattern rather than random noise:\n",
    "\n",
    "* **The Shape:** The errors form a loop structure when plotting *Hour of Day* vs. *Day of Year*. This shape closely mirrors the **Solar Analemma**\u2014the figure-8 path the sun traces in the sky over a year.  \n",
    "* **The \"Hole\" in the Center:** The absence of high errors in the center of the plot indicates that the model performs reliably during \"stable\" solar windows (e.g., mid-day in summer or stable winter low-light), where the relationship between time and irradiance is linear and predictable.  \n",
    "* **The Boundary (The Danger Zone):** The model struggles most at the **edges** of the solar window:  \n",
    "  * **Sunrise/Sunset Gradients:** Rapid changes in air mass and atmospheric scattering occur here.  \n",
    "  * **Shoulder Seasons:** During Spring and Autumn equinoxes, weather volatility is higher, making static training data less representative of daily variance.\n",
    "\n",
    "#### **2\\. Metric Interpretation for Solar Forecasting**\n",
    "\n",
    "| Metric | Value | Technical Context |\n",
    "| :---- | :---- | :---- |\n",
    "| **Testing R\u00b2 (0.4787)** | Moderate | While the model captures the general day/night cycle well, the moderate R\u00b2 on the test set suggests it misses high-frequency variance (e.g., passing clouds). This is typical for models lacking time-series memory (like LSTM) or cloud motion vectors. |\n",
    "| **N-MAE (7.78%)** | Good | A Normalized Mean Absolute Error under 10% is generally considered deployment-ready for grid integration tasks. It means the average prediction error is less than 8% of the system's total capacity. |\n",
    "| **RMSE vs. MAE** | Gap exists | The gap between RMSE (0.04) and MAE (0.03) indicates the presence of outliers. Since RMSE penalizes large errors more heavily, this confirms that while the model is generally accurate, it occasionally misses significantly (likely during the \"Error Loop\" times identified above). |\n",
    "\n",
    "**Recommendation:** To improve performance in the identified \"Error Loop,\" consider engineering features specifically for **Solar Elevation Angle** and **Azimuth**, or separating the model into \"Clear Sky\" vs. \"Cloudy\" sub-models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QQ plot for residuals\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(10,6))\n",
    "stats.probplot(y_test - y_test_pred, dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot of Residuals (Test Data)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc672ad0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80628f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot actual vs predicted for first 200 samples\n",
    "fig, ax = plt.subplots(4,1,figsize=(15, 18))\n",
    "ax = ax.flatten()\n",
    "if target_col == 'PV(W)':\n",
    "    ax[0].plot(y_test[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[0].plot(y_test_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[0].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[0].set_xlabel('Sample Index')\n",
    "    ax[0].set_ylabel('Power Output (W)')\n",
    "    ax[0].legend()\n",
    "    # add a gap between test and train plots\n",
    "    ax[1].plot(y_train[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[1].plot(y_train_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[1].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[1].set_xlabel('Sample Index')\n",
    "    ax[1].set_ylabel('Power Output (W)')\n",
    "    ax[1].legend()\n",
    "if target_col == 'Clearsky_Index':\n",
    "    ax[0].plot(y_test[:200] *  df_daytime_test['Total_Power_ClearSky_Output(W)'][:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[0].plot(y_test_pred[:200] *  df_daytime_test['Total_Power_ClearSky_Output(W)'][:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[0].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[0].set_xlabel('Sample Index')\n",
    "    ax[0].set_ylabel('Power Output (W)')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(y_train[:200] *  df_daytime_train['Total_Power_ClearSky_Output(W)'][:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[1].plot(y_train_pred[:200] *  df_daytime_train['Total_Power_ClearSky_Output(W)'][:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[1].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[1].set_xlabel('Sample Index')\n",
    "    ax[1].set_ylabel('Power Output (W)')\n",
    "    ax[1].legend()\n",
    "\n",
    "    ax[2].plot(y_test[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[2].plot(y_test_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[2].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[2].set_xlabel('Sample Index')\n",
    "    ax[2].set_ylabel(target_col)\n",
    "    ax[2].legend()\n",
    "\n",
    "    ax[3].plot(y_train[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[3].plot(y_train_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[3].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[3].set_xlabel('Sample Index')\n",
    "    ax[3].set_ylabel(target_col)\n",
    "    ax[3].legend()\n",
    "if target_col == 'PV(W)_error' :\n",
    "    ax[0].plot((y_test[:200]+ df_daytime_test['Total_Power_ClearSky_Output(W)'][:200]).clip(lower=0), label=f'Actual  Calculated PV(W) Clipped' , color='blue', alpha=0.7)\n",
    "    ax[0].plot((y_test_pred[:200]+ df_daytime_test['Total_Power_ClearSky_Output(W)'][:200]).clip(lower=0), label='Predicted Calculated PV(W) Clipped', color='red', alpha=0.7)\n",
    "    ax[0].plot(df_daytime_test['PV(W)'][:200], label='Actual PV(W)', color='green', linestyle=':', alpha=0.7)\n",
    "    ax[0].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[0].set_xlabel('Sample Index')\n",
    "    ax[0].set_ylabel('Power Output (W)')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot((y_train[:200] + df_daytime_train['Total_Power_ClearSky_Output(W)'][:200]).clip(lower=0), label=f'Actual Calculated PV(W)  Clipped', color='blue', alpha=0.7)\n",
    "    ax[1].plot((y_train_pred[:200]+ df_daytime_train['Total_Power_ClearSky_Output(W)'][:200]).clip(lower=0), label='Predicted Calculated PV(W)  Clipped', color='red', linestyle='--', alpha=0.7)\n",
    "    ax[1].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[1].set_xlabel('Sample Index')\n",
    "    ax[1].set_ylabel('Power Output (W)')\n",
    "    ax[1].legend()\n",
    "    ax[2].plot(y_test_pvw[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[2].plot(y_test_pvw_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[2].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[2].set_xlabel('Sample Index')\n",
    "    ax[2].set_ylabel(target_col)\n",
    "    #ax[2].legend()\n",
    "    #ax[2].plot(y_test[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    #ax[2].plot(y_test_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    #ax[2].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    #ax[2].set_xlabel('Sample Index')\n",
    "    #ax[2].set_ylabel(target_col)\n",
    "    #ax[2].legend()\n",
    "\n",
    "    ax[3].plot(y_train[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[3].plot(y_train_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[3].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[3].set_xlabel('Sample Index')\n",
    "    ax[3].set_ylabel        \n",
    "if target_col == 'PV(W)_error_index':\n",
    "    ax[0].plot(y_test[:200] *  df_daytime_test['Total_Power_ClearSky_Output(W)'][:200] + df_daytime_test['Total_Power_ClearSky_Output(W)'][:200], label=f'Actual Calculated PV(W)', color='blue', alpha=0.7)\n",
    "    ax[0].plot(y_test_pred[:200] *  df_daytime_test['Total_Power_ClearSky_Output(W)'][:200] + df_daytime_test['Total_Power_ClearSky_Output(W)'][:200], label='Predicted Calculated PV(W)', color='red', linestyle='--', alpha=0.7)\n",
    "    ax[0].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[0].set_xlabel('Sample Index')\n",
    "    ax[0].set_ylabel('Power Output (W)')\n",
    "    ax[0].legend()\n",
    "    ax[1].plot(y_train[:200] *  df_daytime_train['Total_Power_ClearSky_Output(W)'][:200] + df_daytime_train['Total_Power_ClearSky_Output(W)'][:200], label=f'Actual Calculated PV(W)', color='blue', alpha=0.7)\n",
    "    ax[1].plot(y_train_pred[:200] *  df_daytime_train['Total_Power_ClearSky_Output(W)'][:200] + df_daytime_train['Total_Power_ClearSky_Output(W)'][:200], label='Predicted Calculated PV(W)', color='red', linestyle='--', alpha=0.7)\n",
    "    ax[1].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[1].set_xlabel('Sample Index')\n",
    "    ax[1].set_ylabel('Power Output (W)')\n",
    "    ax[1].legend()\n",
    "    ax[2].plot(y_test[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[2].plot(y_test_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[2].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Test Samples)')\n",
    "    ax[2].set_xlabel('Sample Index')\n",
    "    ax[2].set_ylabel(target_col)\n",
    "    ax[2].legend()\n",
    "\n",
    "    ax[3].plot(y_train[:200], label=f'Actual ' + target_col, color='blue', alpha=0.7)\n",
    "    ax[3].plot(y_train_pred[:200], label='Predicted ' + target_col, color='red', linestyle='--', alpha=0.7)\n",
    "    ax[3].set_title('Random Forest: Actual vs Predicted Solar Output (First 200 Training Samples)')\n",
    "    ax[3].set_xlabel('Sample Index')\n",
    "    ax[3].set_ylabel\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4c553",
   "metadata": {},
   "source": [
    "This is too to investigate if using the media as an alternative to mean gives a beter result.\n",
    "\n",
    "\n",
    "Note: Gemini Prompt to generate the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a892b29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# 1. Gather predictions from all individual trees\n",
    "print(\"Gathering predictions from all trees... (this might take a moment)\")\n",
    "# We use .values to avoid the \"feature names\" warning\n",
    "all_tree_preds = np.array([tree.predict(X_test.values) for tree in xgb_model.estimators_])\n",
    "\n",
    "# 2. Calculate Median and Mean Predictions\n",
    "y_pred_median = np.median(all_tree_preds, axis=0)\n",
    "y_pred_mean = xgb_model.predict(X_test)\n",
    "\n",
    "# 3. Calculate Standard Metrics\n",
    "mae_median = mean_absolute_error(y_test, y_pred_median)\n",
    "rmse_median = np.sqrt(mean_squared_error(y_test, y_pred_median))\n",
    "\n",
    "mae_mean = mean_absolute_error(y_test, y_pred_mean)\n",
    "rmse_mean = np.sqrt(mean_squared_error(y_test, y_pred_mean))\n",
    "\n",
    "# 4. Calculate Normalized Metrics\n",
    "# We divide by the average of the actual values (y_test.mean())\n",
    "# You could also use (y_test.max() - y_test.min()) if preferred\n",
    "normalization_factor = y_test.mean()\n",
    "\n",
    "n_mae_median = mae_median / normalization_factor\n",
    "n_rmse_median = rmse_median / normalization_factor\n",
    "\n",
    "n_mae_mean = mae_mean / normalization_factor\n",
    "n_rmse_mean = rmse_mean / normalization_factor\n",
    "\n",
    "# 5. Print Results with Percentages\n",
    "print(f\"\\n--- Model Performance Comparison ---\")\n",
    "print(f\"Normalization Factor (Mean of Actuals): {normalization_factor:.2f} W\")\n",
    "\n",
    "print(f\"\\n1. Standard Random Forest (Mean):\")\n",
    "print(f\"   MAE:    {mae_mean:.2f} W\")\n",
    "print(f\"   nMAE:   {n_mae_mean:.4f} ({n_mae_mean*100:.2f}%)\")\n",
    "print(f\"   RMSE:   {rmse_mean:.2f} W\")\n",
    "print(f\"   nRMSE:  {n_rmse_mean:.4f} ({n_rmse_mean*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\n2. Median Random Forest (Median):\")\n",
    "print(f\"   MAE:    {mae_median:.2f} W\")\n",
    "print(f\"   nMAE:   {n_mae_median:.4f} ({n_mae_median*100:.2f}%)\")\n",
    "print(f\"   RMSE:   {rmse_median:.2f} W\")\n",
    "print(f\"   nRMSE:  {n_rmse_median:.4f} ({n_rmse_median*100:.2f}%)\")\n",
    "\n",
    "# Check which is better\n",
    "if n_mae_median < n_mae_mean:\n",
    "    print(f\"\\n\u2705 Success! The Median method reduced the nMAE by {(n_mae_mean - n_mae_median)*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"\\n\u2139\ufe0f The Standard Mean method was better by {(n_mae_median - n_mae_mean)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9c1c2ac",
   "metadata": {},
   "source": [
    "Investigate what the impact to clipping is on the MAE , RMSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a410b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# --- 1. Generate Base Predictions (if you haven't already) ---\n",
    "# We need the model's raw predictions (the errors/residuals) first\n",
    "y_train_pred = xgb_model.predict(X_train)\n",
    "y_test_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# --- 2. Reconstruct the Total PV Values (Correcting the Typos) ---\n",
    "\n",
    "# Train Set Reconstruction\n",
    "# We add the 'ClearSky' base model back to our residuals to get total Watts\n",
    "y_train_actual_reconstructed = (y_train + df_daytime_train['Total_Power_ClearSky_Output(W)'])\n",
    "y_train_pred_reconstructed   = (y_train_pred + df_daytime_train['Total_Power_ClearSky_Output(W)'])\n",
    "\n",
    "# Test Set Reconstruction (FIXED: Using df_daytime_test and y_test)\n",
    "y_test_actual_reconstructed = (y_test + df_daytime_test['Total_Power_ClearSky_Output(W)'])\n",
    "y_test_pred_reconstructed   = (y_test_pred + df_daytime_test['Total_Power_ClearSky_Output(W)'])\n",
    "\n",
    "# --- 3. Apply Clipping (No Negative Solar Power) ---\n",
    "# It's physically impossible to have negative power, so we clip at 0\n",
    "y_train_actual_clipped = y_train_actual_reconstructed.clip(lower=0)\n",
    "y_train_pred_clipped   = y_train_pred_reconstructed.clip(lower=0)\n",
    "\n",
    "y_test_actual_clipped = y_test_actual_reconstructed.clip(lower=0)\n",
    "y_test_pred_clipped   = y_test_pred_reconstructed.clip(lower=0)\n",
    "\n",
    "\n",
    "# --- 4. The \"Magic\" Function to Calculate & Print Stats ---\n",
    "def print_solar_metrics(y_true, y_pred, label=\"Data\"):\n",
    "    # Basic Metrics\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Normalized Metrics (Dividing by the Mean of the Actual Data)\n",
    "    # This gives us the error relative to the average power output\n",
    "    mean_val = y_true.mean()\n",
    "    n_mae = mae / mean_val\n",
    "    n_rmse = rmse / mean_val\n",
    "    \n",
    "    print(f\"--- Results for: {label} ---\")\n",
    "    print(f\"R\u00b2 Score:   {r2:.4f}\")\n",
    "    print(f\"MAE:        {mae:.2f} W\")\n",
    "    print(f\"RMSE:       {rmse:.2f} W\")\n",
    "    print(f\"nMAE:       {n_mae:.4f}  ({n_mae*100:.2f}%)\")\n",
    "    print(f\"nRMSE:      {n_rmse:.4f} ({n_rmse*100:.2f}%)\")\n",
    "    print(\"-\" * 30 + \"\\n\")\n",
    "\n",
    "# --- 5. Run the Calculations ---\n",
    "print(\"=== UNCLIPPED (Raw Train) ===\")\n",
    "print_solar_metrics(y_train, y_train_pred, label=\"Train Set - Error\")\n",
    "print_solar_metrics(y_test, y_test_pred, label=\"Test Set - Error\")\n",
    "\n",
    "print(\"=== UNCLIPPED (Raw Reconstruction) ===\")\n",
    "print_solar_metrics(y_train_actual_reconstructed, y_train_pred_reconstructed, label=\"Train Set (Unclipped)\")\n",
    "print_solar_metrics(y_test_actual_reconstructed, y_test_pred_reconstructed, label=\"Test Set (Unclipped)\")\n",
    "\n",
    "print(\"=== CLIPPED (Negative Values Removed) ===\")\n",
    "print_solar_metrics(y_train_actual_clipped, y_train_pred_clipped, label=\"Train Set (Clipped)\")\n",
    "print_solar_metrics(y_test_actual_clipped, y_test_pred_clipped, label=\"Test Set (Clipped)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f28f32",
   "metadata": {},
   "source": [
    "Trying to adjust clear sky by a factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5194989e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Select a slice of data (e.g., 5 days from the test set)\n",
    "# We pick a start point and take 120 hours (5 days * 24 hours)\n",
    "start_idx = 0  # Change this to look at different weeks!\n",
    "end_idx   = start_idx + 120\n",
    "\n",
    "subset = df_daytime_test.iloc[start_idx:end_idx]\n",
    "\n",
    "# 2. Setup the Plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# 3. Plot the Curves\n",
    "# The \"Theoretical Maximum\" (PVLIB)\n",
    "plt.plot(subset.index, subset['Total_Power_ClearSky_Output(W)'], \n",
    "         label='PVLIB Clear Sky (Theoretical)', color='orange', linestyle='--', linewidth=2)\n",
    "\n",
    "# The \"Actual Reality\"\n",
    "plt.plot(subset.index, subset['PV(W)'], \n",
    "         label='Actual PV Output', color='blue', linewidth=2)\n",
    "\n",
    "# Optional: Plot your Hybrid Prediction if you have it!\n",
    "# plt.plot(subset.index, y_pred_hybrid[start_idx:end_idx], label='Hybrid Prediction', color='green')\n",
    "\n",
    "# 4. Make it readable\n",
    "plt.title('Reality Check: Theoretical vs. Actual Solar Output', fontsize=16)\n",
    "plt.ylabel('Power (Watts)', fontsize=12)\n",
    "plt.xlabel('Date/Time', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Reconstruct Actual Values (same as before) ---\ntrain_actual_pv = (y_train + df_daytime_train['Total_Power_ClearSky_Output(W)'])\ntrain_clearsky  = df_daytime_train['Total_Power_ClearSky_Output(W)']\n\n# --- 2. Calculate the \"System Efficiency Factor\" ---\nhigh_power_mask = train_actual_pv > train_actual_pv.quantile(0.90)\nperformance_factor = (train_actual_pv[high_power_mask] / train_clearsky[high_power_mask]).median()\n\nprint(f\"\u2600\ufe0f Calculated System Performance Factor: {performance_factor:.4f}\")\nprint(\"This means your system is performing at about {:.1f}% of the theoretical maximum.\".format(performance_factor*100))\n\n# --- 3. Create the NEW Corrected Target ---\nnew_base_train = train_clearsky * performance_factor\nnew_target_train = train_actual_pv - new_base_train\n\nnew_base_test = df_daytime_test['Total_Power_ClearSky_Output(W)'] * performance_factor\ny_test_actual_reconstructed = (y_test + df_daytime_test['Total_Power_ClearSky_Output(W)'])\n\n# --- 4. Train a New XGBoost on this Corrected Target ---\nprint(\"\\nTraining new Calibrated Residual Model...\")\nxgb_calibrated = xgb.XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.1,\n    max_depth=10,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    reg_alpha=0.1,\n    reg_lambda=1.0,\n    random_state=42,\n    n_jobs=-1\n)\nxgb_calibrated.fit(X_train, new_target_train)\n\n# --- 5. Predict and Evaluate ---\ny_pred_new_error = xgb_calibrated.predict(X_test)\ny_pred_final_calibrated = (y_pred_new_error + new_base_test).clip(lower=0)\n\n# Calculate Metrics\nrmse_calib = np.sqrt(mean_squared_error(y_test_actual_reconstructed, y_pred_final_calibrated))\nn_rmse_calib = rmse_calib / y_test_actual_reconstructed.mean()\nr2_calib = r2_score(y_test_actual_reconstructed, y_pred_final_calibrated)\n\nprint(f\"\\n--- Results for Calibrated Model (Factor: {performance_factor:.2f}) ---\")\nprint(f\"R\u00b2 Score:  {r2_calib:.4f}\")\nprint(f\"RMSE:      {rmse_calib:.2f} W\")\nprint(f\"nRMSE:     {n_rmse_calib:.4f} ({n_rmse_calib*100:.2f}%)\")\n"
   ],
   "id": "calibrated_model"
  },
  {
   "cell_type": "markdown",
   "id": "a69f4fbe",
   "metadata": {},
   "source": [
    "Write results to CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'results/xgboost_regressor_hourly_test_metrics.csv', 'a') as f:\n    # if this is first line write header\n    if os.stat(f'results/xgboost_regressor_hourly_test_metrics.csv').st_size == 0:\n        header = ['Model', 'Test No', 'Target Column',\n                  'Test Name', 'Test RMSE', 'Test MAE', 'Test R2', 'Test N-RMSE', 'Test N-MAE','Test N-RMSE %', 'Test N-MAE %', 'Test Accuracy',\n                  'Train RMSE', 'Train MAE', 'Train R2', 'Train N-RMSE', 'Train N-MAE', 'Train Accuracy','Train N-RMSE %', 'Train N-MAE %', \n                  'Test PV(W) RMSE', 'Test PV(W) MAE', 'Test PV(W) R2', 'Test PV(W) N-RMSE', 'Test PV(W) N-MAE','Test PV(W) N-RMSE %', 'Test PV(W) N-MAE %', 'Test PV(W) Accuracy',\n                  'Train PV(W) RMSE', 'Train PV(W) MAE', 'Train PV(W) R2', 'Train PV(W) N-RMSE', 'Train PV(W) N-MAE', 'Train PV(W) Accuracy','Train PV(W) N-RMSE %', 'Train PV(W) N-MAE %',\n                  'Notes', 'Feature Columns']\n        f.write(','.join(header) + '\\n')\n    line=[]\n    line.append(\"XGBoost Regressor Hourly\")\n    line.append(test_no)\n    line.append(test_name)\n    line.append(f\"{target_col}\")\n    line.append(f\"{test_rmse:.2f}\")\n    line.append(f\"{test_mae:.2f}\")\n    line.append(f\"{test_r2:.4f}\")\n    line.append(f\"{test_n_rmse:.4f}\")\n    line.append(f\"{test_n_mae:.4f}\")\n    line.append(f\"{test_n_rmse*100:.4f}\")\n    line.append(f\"{test_n_mae*100:.4f}\")\n    line.append(f\"{test_accuracy:.4f}\")\n    line.append(f\"{train_rmse:.2f}\")\n    line.append(f\"{train_mae:.2f}\")\n    line.append(f\"{train_r2:.4f}\")\n    line.append(f\"{train_n_rmse:.4f}\")\n    line.append(f\"{train_n_mae:.4f}\")\n    line.append(f\"{train_n_rmse*100:.4f}\")\n    line.append(f\"{train_n_mae*100:.4f}\")\n    line.append(f\"{train_accuracy:.4f}\")\n    line.append(f\"{test_rmse_pvw:.2f}\")\n    line.append(f\"{test_mae_pvw:.2f}\")\n    line.append(f\"{test_r2_pvw:.4f}\")\n    line.append(f\"{test_n_rmse_pvw:.4f}\")\n    line.append(f\"{test_n_mae_pvw:.4f}\")\n    line.append(f\"{test_n_rmse_pvw*100:.4f}\")\n    line.append(f\"{test_n_mae_pvw*100:.4f}\")\n    line.append(f\"{test_accuracy_pvw:.4f}\")\n    line.append(f\"{train_rmse_pvw:.2f}\")\n    line.append(f\"{train_mae_pvw:.2f}\")\n    line.append(f\"{train_r2_pvw:.4f}\")\n    line.append(f\"{train_n_rmse_pvw:.4f}\")\n    line.append(f\"{train_n_mae_pvw:.4f}\")\n    line.append(f\"{train_n_rmse_pvw*100:.4f}\")\n    line.append(f\"{train_n_mae_pvw*100:.4f}\")\n    line.append(f\"{train_accuracy_pvw:.4f}\")\n    line.append(f\"{notes if 'notes' in locals() else ''}\")\n    line.append(f\"feature_cols: {':'.join(feature_cols)}\")\n    f.write(','.join(line) + '\\n')\n"
   ],
   "id": "results_output"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}