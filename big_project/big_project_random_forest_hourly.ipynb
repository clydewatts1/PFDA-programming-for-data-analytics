{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c0391",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "# xlrd is required for reading xls Excel files\n",
    "import xlrd\n",
    "import re\n",
    "import sqlite3\n",
    "import meteostat as mt\n",
    "# Use this when displaying markdown in Jupyter Notebooks ( Gemini suggestion )\n",
    "from IPython.display import display, Markdown\n",
    "# do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aec113",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_nighlty_threshold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864beae2",
   "metadata": {},
   "source": [
    "# Random Forest Regresion Analysis Of Solar and Weather\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34515210",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_enriched_features = \"data/processed_data/hourly_solar_copernicus_enriched_data.feather\"\n",
    "file_weather_data = \"data/processed_data/hourly_weather_data.feather\"\n",
    "file_solar_data=\"data/processed_data/daily_solar_data.feather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddce1401",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched_features=pd.read_feather(file_enriched_features)\n",
    "df_weather_data = pd.read_feather(file_weather_data)\n",
    "df_solar_data = pd.read_feather(file_solar_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4670de",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_solar_per_hour = df_solar_data[['PV(W)','DateTime']].resample('h', on='DateTime').sum()\n",
    "# \n",
    "df_solar_data[df_solar_data['DateTime'].dt.date == datetime.date(2024,12,20)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5275d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the columns/features for each dataset using pandas\n",
    "sns.set_theme()\n",
    "feature_columns = {\n",
    "    \"Enriched Features\": df_enriched_features.columns,\n",
    "    \"Weather Data\": df_weather_data.columns,\n",
    "    \"Solar Data\": df_solar_data.columns,\n",
    "}\n",
    "for title, cols in feature_columns.items():\n",
    "    display(pd.DataFrame({title: cols}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f96cc5",
   "metadata": {},
   "source": [
    "Convert to hourly date time granularity\n",
    "\n",
    "This is too see if I can get more accurate readings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6c37b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change all the 3  to an hourly aggregation using resample\n",
    "print(df_enriched_features.columns.to_list())\n",
    "df_solar_per_hour = df_solar_data[['PV(W)','DateTime']].resample('h', on='DateTime').agg('mean')\n",
    "df_weather_per_hour = df_weather_data.resample('h', on='DateTime').agg({'Temperature(C)': 'mean',\n",
    "                                                                     'Humidity(%)': 'mean',\n",
    "                                                                     'Sunshine Duration': 'mean',\n",
    "                                                                     'Condition Code': 'first',})\n",
    "# 'DateTime', '# Observation period', 'TOA', 'Clear sky GHI', 'Clear sky BHI', 'Clear sky DHI', 'Clear sky BNI', 'GHI', 'BHI', 'DHI', 'BNI', 'Reliability,', 'Time', 'Date', 'POA_Pane_I(W/m^2)', 'POA_Pane_II(W/m^2)', 'POAC_Pane_I(W/m^2)', 'POAC_Pane_II(W/m^2)', 'Power_Pane_I(W)', 'Power_Pane_II(W)', 'Power_ClearSky_Pane_I(W)', 'Power_ClearSky_Pane_II(W)', 'Total_Power_Output(W)', 'Total_Power_ClearSky_Output(W)'\n",
    "df_enriched_per_hour = df_enriched_features.resample('h', on='DateTime').agg({\n",
    "    'TOA': 'sum',\n",
    "    'Clear sky GHI': 'sum',\n",
    "    'Clear sky BHI': 'sum',\n",
    "    'Clear sky DHI': 'sum',\n",
    "    'GHI': 'sum',\n",
    "    'BHI': 'sum',\n",
    "    'DHI': 'sum',\n",
    "    'BNI': 'sum',\n",
    "    'POA_Pane_I(W/m^2)': 'sum',\n",
    "    'POA_Pane_II(W/m^2)': 'sum',\n",
    "    'POAC_Pane_I(W/m^2)': 'sum',\n",
    "    'POAC_Pane_II(W/m^2)': 'sum',\n",
    "    'Power_Pane_I(W)': 'sum',\n",
    "    'Power_Pane_II(W)': 'sum',\n",
    "    'Power_ClearSky_Pane_I(W)': 'sum',\n",
    "    'Power_ClearSky_Pane_II(W)': 'sum',\n",
    "    'Total_Power_Output(W)': 'sum',\n",
    "    'Total_Power_ClearSky_Output(W)': 'sum'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9973131",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_hourly = df_solar_per_hour.merge(df_weather_per_hour, on='DateTime').merge(df_enriched_per_hour, on='DateTime')\n",
    "df_merge_hourly.reset_index(inplace=True)\n",
    "# Add WeekOfYear\n",
    "df_merge_hourly['WeekOfYear'] = df_merge_hourly['DateTime'].dt.isocalendar().week\n",
    "df_merge_hourly['Month_Sin'] = np.sin(2 * np.pi * (df_merge_hourly['DateTime'].dt.month - 1) / 12)\n",
    "df_merge_hourly['DayOfYear_Sin'] = np.sin(2 * np.pi * (df_merge_hourly['DateTime'].dt.dayofyear - 1) / 365)\n",
    "df_merge_hourly['HourOfDay_Sin'] = np.sin(2 * np.pi * (df_merge_hourly['DateTime'].dt.hour) / 24)\n",
    "# Count number Nan in target Y column\n",
    "print(\"Number of NaN in PV(W):\", df_merge_hourly['PV(W)'].isna().sum())\n",
    "# Remove Nan in target Y column\n",
    "df_merge_hourly = df_merge_hourly.dropna(subset=['PV(W)'])\n",
    "# Count number of rows after removing NaN\n",
    "print(\"Number of NaN  after removing in PV(W):\", df_merge_hourly['PV(W)'].isna().sum())\n",
    "print(\"Number of rows after removing NaN in PV(W):\", len(df_merge_hourly))\n",
    "print(\"Dataframe shape after removing NaN in PV(W):\", df_merge_hourly.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c61aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display column list and data types and index\n",
    "display(pd.DataFrame({\"Columns\": df_merge_hourly.columns, \"Data Types\": df_merge_hourly.dtypes}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214cf659",
   "metadata": {},
   "source": [
    "## Plot the solar output to determine cutoff points\n",
    "\n",
    "The objective is to visually examine the output and determine the threshold for night time. It is more accurrate to have a rule that determines that there is no power , and not train the model on this. it skews the accuracy. This was from a gemini prompt to assist in building the model. The Clear SKY GDI shows the output based on clear sky , even at night because of abient light from street lights , and full moon there is still power generation at night. This system can generate up to 300 W on a very bright full moon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cc8f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the hour of day vs max , min , mean of PV(W)\n",
    "fig, ax = plt.subplots(4,1 , figsize=(15 , 12) )\n",
    "# flatten ax\n",
    "ax = ax.flatten()\n",
    "plt.grid()\n",
    "# plot hour of day vs min, max , mean of PV(W)\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='PV(W)', estimator='mean', ax=ax[0])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='PV(W)', estimator='max', ax=ax[0])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='PV(W)', estimator='min', ax=ax[0])\n",
    "# plot the mean line below hourly_nighlty_threshold with dashed line\n",
    "#sns.lineplot(data=df_merge_hourly[df_merge_hourly['PV(W)'] < hourly_nighlty_threshold], x=df_merge_hourly['DateTime'].dt.hour, y='PV(W)', estimator='mean', ax=ax[0], linestyle='--')\n",
    "# draw a red vertical line at sunrise and sunset hours (4 and 22)\n",
    "ax[0].axvline(x=4, color='red', linestyle='--', label='Sunrise')\n",
    "ax[0].axvline(x=22, color='orange', linestyle='--', label='Sunset')\n",
    "ax[0].set_title('Hour of Day vs PV(W) Mean, Max, Min')\n",
    "ax[0].set_xlabel('Hour of Day')\n",
    "# Same plot for Solar Radiation\n",
    "ax[1].set_title('Hour of Day vs Sunshine Duration Mean')\n",
    "ax[1].set_xlabel('Hour of Day')\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Sunshine Duration', estimator='mean', ax=ax[1])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Sunshine Duration', estimator='max', ax=ax[1])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Sunshine Duration', estimator='min', ax=ax[1])\n",
    "# draw a red vertical line at sunrise and sunset hours (4 and 22)\n",
    "ax[1].axvline(x=4, color='red', linestyle='--', label='Sunrise')\n",
    "ax[1].axvline(x=22, color='orange', linestyle='--', label='Sunset')\n",
    "ax[2].set_title('Hour of Day vs GHI Mean')\n",
    "ax[2].set_xlabel('Hour of Day')\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='GHI', estimator='mean', ax=ax[2])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='GHI', estimator='max', ax=ax[2])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='GHI', estimator='min', ax=ax[2])\n",
    "# draw a red vertical line at sunrise and sunset hours (4 and 22)\n",
    "ax[2].axvline(x=4, color='red', linestyle='--', label='Sunrise')\n",
    "ax[2].axvline(x=22, color='orange', linestyle='--', label='Sunset')\n",
    "ax[3].set_title('Hour of Day vs Clear sky GHI Mean')\n",
    "ax[3].set_xlabel('Hour of Day')\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Clear sky GHI', estimator='mean', ax=ax[3])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Clear sky GHI', estimator='max', ax=ax[3])\n",
    "sns.lineplot(data=df_merge_hourly, x=df_merge_hourly['DateTime'].dt.hour, y='Clear sky GHI', estimator='min', ax=ax[3])\n",
    "# draw a red vertical line at sunrise and sunset hours (4 and 22)\n",
    "ax[3].axvline(x=4, color='red', linestyle='--', label='Sunrise')\n",
    "ax[3].axvline(x=22, color='orange', linestyle='--', label='Sunset')\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()# save the merged dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1894d27b",
   "metadata": {},
   "source": [
    "Look at only hours between 4 and 22 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c96674",
   "metadata": {},
   "source": [
    "## Histogram of various solar measures \n",
    "\n",
    "This shows a histogram of the various measures solar output. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea5d208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8907d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", df_merge_hourly.columns.tolist())\n",
    "feature_cols = ['Temperature(C)', 'Humidity(%)', 'Sunshine Duration', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin', 'GHI',\n",
    "                'Total_Power_Output(W)', 'Power_Pane_I(W)', 'Power_Pane_II(W)','Clear sky GHI',\n",
    "                'Total_Power_ClearSky_Output(W)','Power_ClearSky_Pane_I(W)','Power_ClearSky_Pane_II(W)']\n",
    "target_col = 'PV(W)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5e8059",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Columns:\", df_merge_hourly.columns.tolist())#\n",
    "#feature_cols = ['Temperature(C)', 'Humidity(%)', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin',\n",
    "#                'Total_Power_ClearSky_Output(W)','Power_ClearSky_Pane_I(W)','Power_ClearSky_Pane_II(W)']\n",
    "feature_cols = ['Temperature(C)', 'Humidity(%)', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin','Total_Power_Output(W)']\n",
    "target_col = 'PV(W)'\n",
    "print(f\"\\nTesting Random Forest Regressor with target: {target_col} and features: {feature_cols}\")\n",
    "# first we filter  out where the output is less than threshold - to avoid night time data , this is based on clearsky GHI\n",
    "df_daytime = df_merge_hourly['Clear sky GHI'] > 10\n",
    "# now split into test and training based test and train data sets are every other week\n",
    "df_daytime_train = df_merge_hourly[(df_merge_hourly['WeekOfYear'] % 2 == 0) & (df_daytime)]\n",
    "df_daytime_test = df_merge_hourly[(df_merge_hourly['WeekOfYear'] % 2 == 1) & (df_daytime)]\n",
    "print(\"Training DataFrame shape:\", df_daytime_train.shape)\n",
    "print(\"Testing DataFrame shape:\", df_daytime_test.shape)\n",
    "# Create x and y for training and testing\n",
    "X_train = df_daytime_train[feature_cols]\n",
    "y_train = df_daytime_train[target_col]\n",
    "X_test = df_daytime_test[feature_cols]\n",
    "y_test = df_daytime_test[target_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1178be",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42,max_depth=16,min_samples_split=5,min_samples_leaf=2)\n",
    "# Train the model\n",
    "print(\"Fitting model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "print(\"Making predictions on test set... Done\")\n",
    "# making predictions on training set\n",
    "print(\"Making predictions on training set...\")\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "print(\"Making predictions on training set... Done\")\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de79b9cd",
   "metadata": {},
   "source": [
    "__Metrics for Tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df434e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 main metrics: RMSE, MAE, R2 for both training and testing\n",
    "# # do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # and normalized based on MAX of y_true - the standard practice for solar prediction\n",
    "    #normalization_factor = np.max(y_true) \n",
    "    normalization_factor = 8400 # based on max PV(W) based on system size\n",
    "    n_rmse = rmse / normalization_factor\n",
    "    n_mae = mae / normalization_factor\n",
    "    # calculate accuracy as 1 - (mae / mean of y_true)\n",
    "    accuracy_score = 1 - (mae / np.mean(y_true))\n",
    "    return rmse, mae, r2 , n_rmse, n_mae, accuracy_score\n",
    "train_rmse, train_mae, train_r2, train_n_rmse, train_n_mae, train_accuracy = calculate_metrics(y_train, y_train_pred)\n",
    "test_rmse, test_mae, test_r2, test_n_rmse, test_n_mae, test_accuracy = calculate_metrics(y_test, y_test_pred)\n",
    "# print the metrics , output into a markdown table , include percentages for n_rmse and n_mae and accuracy\n",
    "mk_string = \"\"\"| Metric       | Training Set        | Testing Set         |\n",
    "|--------------|---------------------|---------------------|\n",
    "| RMSE         | {:.2f}              | {:.2f}              |\n",
    "| MAE          | {:.2f}              | {:.2f}              |\n",
    "| R2           | {:.4f}              | {:.4f}              |\n",
    "| N-RMSE (%)   | {:.4f}              | {:.4f}              |\n",
    "| N-MAE (%)    | {:.4f}              | {:.4f}              |\n",
    "| Accuracy (%) | {:.4f}              | {:.4f}              |\"\"\".format(\n",
    "    train_rmse, test_rmse,\n",
    "    train_mae, test_mae,\n",
    "    train_r2, test_r2,\n",
    "    train_n_rmse*100, test_n_rmse*100,\n",
    "    train_n_mae*100, test_n_mae*100,\n",
    "    train_accuracy*100, test_accuracy*100\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"### Random Forest Regressor Performance Metrics\\n\" + mk_string))\n",
    "#print(f\"\\nTraining Metrics:\\nRMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, R2: {train_r2:.4f}, N-RMSE: {train_n_rmse:.4f}, N-MAE: {train_n_mae:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "#print(f\"\\nTesting Metrics:\\nRMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, R2: {test_r2:.4f}, N-RMSE: {test_n_rmse:.4f}, N-MAE: {test_n_mae:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "# feature importance plot\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442628e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, y_test - y_pred, color='blue', label='Test Data', alpha=0.5)\n",
    "plt.scatter(y_train, y_train - y_train_pred, color='green', label='Train Data', alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Actual PV(W)')\n",
    "plt.xlabel('Actual PV(W)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97be5fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot risduals per hour of day\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df_daytime_test['DateTime'].dt.hour, y_test - y_pred, color='blue', label='Test Data', alpha=0.5)\n",
    "plt.scatter(df_daytime_train['DateTime'].dt.hour, y_train - y_train_pred\n",
    ", color='green', label='Train Data', alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f1f495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual histogram\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(y_test - y_pred, color='blue', label='Test Data', kde=True, stat=\"density\", bins=30)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.histplot(y_train - y_train_pred, color='green', label='Train Data', kde=True, stat=\"density\", bins=30)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5a308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add QQ plot for residuals\n",
    "import scipy.stats as stats\n",
    "plt.figure(figsize=(10,6))\n",
    "stats.probplot(y_test - y_pred, dist=\"norm\", plot=plt)\n",
    "plt.title('QQ Plot of Residuals (Test Data)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac442a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.parsers.expat import errors\n",
    "import scipy.stats as stats\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import randint \n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42,max_depth=16,min_samples_split=5,min_samples_leaf=2)\n",
    "# Train the model\n",
    "print(\"Fitting model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"Making predictions on test set... Done\")\n",
    "# making predictions on training set\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = rf_model.predict(X_test)\n",
    "param_dist = {\n",
    "  'n_estimators': randint(100, 500),\n",
    "  'max_depth': randint(3, 15),\n",
    "  'min_samples_split': randint(2, 10),\n",
    "  'min_samples_leaf': randint(1, 5)\n",
    "}\n",
    "rf_random_search = RandomizedSearchCV(estimator=rf_model, param_distributions=param_dist,\n",
    "                                      n_iter=10, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "# Fit the random search model\n",
    "rf_random_search.fit(X_train, y_train)\n",
    "print(\"Best parameters found: \", rf_random_search.best_params_)\n",
    "# Evaluate the mod\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f187f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 main metrics: RMSE, MAE, R2 for both training and testing\n",
    "# # do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Modelling\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    # and normalized based on MAX of y_true - the standard practice for solar prediction\n",
    "    #normalization_factor = np.max(y_true) \n",
    "    normalization_factor = 8400 # based on max PV(W) based on system size\n",
    "    n_rmse = rmse / normalization_factor\n",
    "    n_mae = mae / normalization_factor\n",
    "    # calculate accuracy as 1 - (mae / mean of y_true)\n",
    "    accuracy_score = 1 - (mae / np.mean(y_true))\n",
    "    return rmse, mae, r2 , n_rmse, n_mae, accuracy_score\n",
    "train_rmse, train_mae, train_r2, train_n_rmse, train_n_mae, train_accuracy = calculate_metrics(y_train, y_train_pred)\n",
    "test_rmse, test_mae, test_r2, test_n_rmse, test_n_mae, test_accuracy = calculate_metrics(y_test, y_test_pred)\n",
    "# print the metrics , output into a markdown table , include percentages for n_rmse and n_mae and accuracy\n",
    "mk_string = \"\"\"| Metric       | Training Set        | Testing Set         |\n",
    "|--------------|---------------------|---------------------|\n",
    "| RMSE         | {:.2f}              | {:.2f}              |\n",
    "| MAE          | {:.2f}              | {:.2f}              |\n",
    "| R2           | {:.4f}              | {:.4f}              |\n",
    "| N-RMSE (%)   | {:.4f}              | {:.4f}              |\n",
    "| N-MAE (%)    | {:.4f}              | {:.4f}              |\n",
    "| Accuracy (%) | {:.4f}              | {:.4f}              |\"\"\".format(\n",
    "    train_rmse, test_rmse,\n",
    "    train_mae, test_mae,\n",
    "    train_r2, test_r2,\n",
    "    train_n_rmse*100, test_n_rmse*100,\n",
    "    train_n_mae*100, test_n_mae*100,\n",
    "    train_accuracy*100, test_accuracy*100\n",
    ")\n",
    "from IPython.display import display, Markdown\n",
    "display(Markdown(\"### Random Forest Regressor Performance Metrics\\n\" + mk_string))\n",
    "#print(f\"\\nTraining Metrics:\\nRMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}, R2: {train_r2:.4f}, N-RMSE: {train_n_rmse:.4f}, N-MAE: {train_n_mae:.4f}, Accuracy: {train_accuracy:.4f}\")\n",
    "#print(f\"\\nTesting Metrics:\\nRMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, R2: {test_r2:.4f}, N-RMSE: {test_n_rmse:.4f}, N-MAE: {test_n_mae:.4f}, Accuracy: {test_accuracy:.4f}\")\n",
    "# feature importance plot\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "importances = rf_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_cols, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5186382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot residuals\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(y_test, y_test - y_pred, color='blue', label='Test Data', alpha=0.5)\n",
    "plt.scatter(y_train, y_train - y_train_pred, color='green', label='Train Data', alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Actual PV(W)')\n",
    "plt.xlabel('Actual PV(W)')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878c58cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot risduals per hour of day\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(df_daytime_test['DateTime'].dt.hour, y_test - y_pred, color='blue', label='Test Data', alpha=0.5)\n",
    "plt.scatter(df_daytime_train['DateTime'].dt.hour, y_train - y_train_pred\n",
    ", color='green', label='Train Data', alpha=0.5)\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.title('Residuals vs Hour of Day')\n",
    "plt.xlabel('Hour of Day')\n",
    "plt.ylabel('Residuals (Actual - Predicted)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
