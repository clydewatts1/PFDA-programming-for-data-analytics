{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6582ed59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f7502d6c",
   "metadata": {},
   "source": [
    "# Project : 25-26: 4369 -- PROGRAMMING FOR DATA ANALYTICS\n",
    "\n",
    "\n",
    "__Author    : Clyde Watts__  \n",
    "__Lecturere : Andrew Beaty__  \n",
    "__Date      : 2025-11-20__\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eba2533",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea3fd465",
   "metadata": {},
   "source": [
    "## Introduction ##\n",
    "\n",
    "This project will examine the data from a residintial solor system , and do some analysis.\n",
    "\n",
    "The following data sets will be used\n",
    "\n",
    "| Data Source | Description | File Type |\n",
    "|:---|:---:|---:|\n",
    "| Solis Inverter | This report contains various power KWh measurement for a day at an hour grain  | xls |\n",
    "| Solis Inverter | This report contains various power KWh measurement for a month at an daily grain  | xls |\n",
    "| Solis Inverter | This report contains various power KWh measurement for a year at an monthly grain  | xls |\n",
    "| Bord Gas / ESB | This report contains daily export to grid | csv |\n",
    "| MetEirn ? | Dublin Airport Weither  | csv / url |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656ed8e3",
   "metadata": {},
   "source": [
    "__Imports__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9487e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "# xlrd is required for reading xls Excel files\n",
    "import xlrd\n",
    "import re\n",
    "import sqlite3\n",
    "import meteostat as mt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b49996",
   "metadata": {},
   "source": [
    "__Globals__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9778e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the current path of the notebook\n",
    "notebook_path = os.path.abspath(\"big_project.ipynb\")\n",
    "notebook_dir = os.path.dirname(notebook_path).replace('\\\\', '/')\n",
    "print(\"Current notebook directory:\", notebook_dir)\n",
    "HOME_DIR = f'{notebook_dir}'\n",
    "DATA_DIR = f'{HOME_DIR}/data/'\n",
    "print(\"Data directory set to:\", DATA_DIR)\n",
    "RAW_DATA_DIR = f'{DATA_DIR}/raw_data/'\n",
    "SQL_DB_PATH = f'{DATA_DIR}/db_sqlite/'\n",
    "SQL_DB_FILE = f'{SQL_DB_PATH}/big_project_db.sqlite3'\n",
    "# Plotly setup\n",
    "plt.style.use('classic')\n",
    "sns.set_style('whitegrid')\n",
    "# Meteostat setup\n",
    "METEOSTAT_CACHE_DIR = f'{DATA_DIR}/meteostat_cache/'\n",
    "SOLAR_SITE_POSITION = (53.6985, -6.2080)  # Bettystown, Ireland\n",
    "LATITUDE, LONGITUDE = SOLAR_SITE_POSITION\n",
    "WEATHER_START_DATE = datetime.datetime(2024, 1, 1)\n",
    "WEATHER_END_DATE = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e66fa3",
   "metadata": {},
   "source": [
    "__Helper Functions__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e61a7a",
   "metadata": {},
   "source": [
    "[sqlite3 table schema](https://sqlite.org/faq.html#:~:text=If%20you%20are%20running%20the,including%20all%20tables%20and%20indices.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afb396",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sqlite3_table_schema(conn, table_name):\n",
    "    \"\"\"\n",
    "    Connects to the SQLite database at db_path and retrieves the schema of the specified table_name.\n",
    "    Prints the schema in a formatted table.\n",
    "    \"\"\"\n",
    "    csr = conn.cursor()\n",
    "    csr.execute(f\"PRAGMA table_info({table_name})\")\n",
    "    columns = csr.fetchall()\n",
    "\n",
    "    print(f\"--- Schema for '{table_name}' ---\")\n",
    "    print(f\"{'ID':<5} {'Name':<15} {'Type':<10} {'NotNull':<10}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    for col in columns:\n",
    "        cid, name, dtype, notnull, dflt_value, pk = col\n",
    "        print(f\"{cid:<5} {name:<15} {dtype:<10} {notnull:<10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8569af1",
   "metadata": {},
   "source": [
    "## Data Loads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f32041",
   "metadata": {},
   "source": [
    "__Database Setup__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be94724",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Ensuring database directory exists...\")\n",
    "try:\n",
    "    os.makedirs(f'{DATA_DIR}/db_sqlite/', exist_ok=True)\n",
    "    print(\"Database directory ensured at:\", f'{DATA_DIR}/db_sqlite/')\n",
    "except Exception as e:\n",
    "    print(\"Error creating database directory:\", e)\n",
    "print(\"Connecting to SQLite database...\")\n",
    "con = sqlite3.connect(SQL_DB_FILE)\n",
    "sys_cur = con.cursor()\n",
    "print(\"Connected to SQLite database at:\", SQL_DB_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e5b8aa",
   "metadata": {},
   "source": [
    "### Solis Data Load\n",
    "\n",
    "The solis data is manually extracted from the solas cloud client interface. This application requires a user logon and password\n",
    "\n",
    "[Solis](https://www.soliscloud.com/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8aad34",
   "metadata": {},
   "source": [
    "### Load and Populate Solar Panel Data ###\n",
    "\n",
    "The solar panel data is the yearly , monthly and daily inverter data from home solar system.  This data is retrieved from solas web site . The web site is a user logon website. The data had to be extracted manually , a period at a time. \n",
    "\n",
    "The data is placed in the directory ./big_project/data/raw_data/solar. \n",
    "\n",
    "[Home Solar System Website](https://www.soliscloud.com/station/stationDetails/generalSituation/1298491919449681542?glyun_vue2=%2F%23%2Fstation) \n",
    "\n",
    "\n",
    "[pandas read excel](https://pandas.pydata.org/docs/reference/api/pandas.read_excel.html) \n",
    "\n",
    "\n",
    "[xlrd](https://pypi.org/project/xlrd/) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f99c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load daily solar data\n",
    "all_dfs = []\n",
    "for file in glob.glob(f'{RAW_DATA_DIR}/solar/Daily*xls'):\n",
    "    df_header = pd.read_excel(file, nrows=5, header=None)\n",
    "    # get date from header\n",
    "    the_date = None\n",
    "    the_date_label = df_header.iat[0,0]\n",
    "    # format of date is Plant_12-09-2024Chart\n",
    "    result = re.search(r'(\\d{2}-\\d{2}-\\d{4})', str(the_date_label))\n",
    "    if result:\n",
    "        date_str = result.group(1)\n",
    "        the_date = datetime.datetime.strptime(date_str, '%d-%m-%Y').date()\n",
    "    else:\n",
    "        print(\"DQ Issue: Date not found in header\")\n",
    "        break\n",
    "    # Get Total Yield from header oday Yield(kWh):29.500kWh\n",
    "    total_yield_label = df_header.iat[3,0]\n",
    "    result_yield = re.search(r'oday Yield\\(kWh\\):([\\d\\.]+)kWh', str(total_yield_label))\n",
    "    if result_yield:\n",
    "        total_yield = float(result_yield.group(1))\n",
    "    # The file needs to be read from line 29\n",
    "    df = pd.read_excel(file,skiprows=28)\n",
    "    # add insert date as first column\n",
    "    df.insert(0, 'Date', the_date)\n",
    "    # The total yield as last column\n",
    "    df['Total_Yield(kWh)'] = total_yield\n",
    "    # add file name column in case there are duplicates\n",
    "    df['Source_File'] = os.path.basename(file)\n",
    "    all_dfs.append(df)\n",
    "# Combine all at once\n",
    "df_raw_daily_solar = pd.concat(all_dfs, ignore_index=True)\n",
    "# remove all_dfs to free memory\n",
    "del all_dfs\n",
    "print(\"Loaded solar data shape:\", df_raw_daily_solar.shape)\n",
    "# Convert to Time column to datetime.time 12:23:45 format\n",
    "df_raw_daily_solar['Time'] = pd.to_datetime(df_raw_daily_solar['Time'], format='%H:%M:%S').dt.time\n",
    "# Combine Date and Time into a single DateTime column\n",
    "df_raw_daily_solar['DateTime'] = df_raw_daily_solar.apply(lambda row: datetime.datetime.combine(row['Date'], row['Time']), axis=1)\n",
    "\n",
    "\n",
    "# Display first few rows\n",
    "df_raw_daily_solar.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a15916",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2ee8d4c",
   "metadata": {},
   "source": [
    "__Write to CSV and Database\n",
    "\n",
    "[to_csv](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_csv.html)\n",
    "[to_sql](https://pandas.pydata.org/pandas-docs/version/2.1.3/reference/api/pandas.DataFrame.to_sql.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing daily solar data to CSV...\")\n",
    "df_raw_daily_solar.to_csv(f'{DATA_DIR}/processed_data/df_raw_daily_solar.csv', index=False)\n",
    "print(f\"Wrote {len(df_raw_daily_solar)} rows to CSV.\")\n",
    "# Write to SQLite database\n",
    "print(\"Writing daily solar data to SQLite database...\")\n",
    "row_count = df_raw_daily_solar.to_sql('daily_solar_data', con, if_exists='replace', index=True)\n",
    "print(f\"Wrote {row_count} rows to SQLite database.\")\n",
    "# Now do a short query to verify\n",
    "test_query = \"SELECT COUNT(*) FROM daily_solar_data;\"\n",
    "sys_cur.execute(test_query)\n",
    "result = sys_cur.fetchone()\n",
    "print(f\"Verified {result[0]} rows in SQLite database.\")\n",
    "sqlite3_table_schema(con, 'daily_solar_data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcb3dff",
   "metadata": {},
   "source": [
    "__Validate Solar dataframe__\n",
    "\n",
    "Check for the following\n",
    "1. Shape\n",
    "2. Columns\n",
    "3. Any dates missing\n",
    "4. Row count per day"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e8a014",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e463f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined DataFrame shape:\", df_raw_daily_solar.shape)\n",
    "print(\"DataFrame info:\")\n",
    "df_raw_daily_solar.info()\n",
    "print(\"Columns:\", df_raw_daily_solar.columns.tolist())\n",
    "df_dates = df_raw_daily_solar['Date'].unique()\n",
    "print(f\"Dates Min: {min(df_dates)}\")\n",
    "print(f\"Dates Max: {max(df_dates)}\")\n",
    "print(\"Total unique dates in data:\", len(df_dates))\n",
    "print(\"Date Range In Days:\", (max(df_dates) - min(df_dates)).days)\n",
    "# Delete all rows after 2025-10-31\n",
    "df_raw_daily_solar = df_raw_daily_solar[df_raw_daily_solar['Date'] <= datetime.date(2025, 10, 31)]\n",
    "# Adjusted shape after filtering dates\n",
    "print(\"DataFrame shape after filtering dates:\", df_raw_daily_solar.shape)\n",
    "# find missing dates\n",
    "# create a complete date range\n",
    "all_dates = pd.date_range(start=min(df_dates), end=max(df_dates))\n",
    "# get the difference between all_dates and df_dates\n",
    "missing_dates = all_dates.difference(df_dates)\n",
    "print(\"Missing dates:\", missing_dates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015da229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a aggregation to daily level\n",
    "df_date_counts = df_raw_daily_solar['Date'].value_counts().reset_index()\n",
    "df_date_counts.columns = ['Date', 'Record_Count']\n",
    "df_date_counts = df_date_counts.sort_values(by='Date')\n",
    "# Plot the daily record counts\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.barplot(data=df_date_counts, x='Date', y='Record_Count', color='blue')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Daily Record Counts')\n",
    "plt.show()\n",
    "# print outliers where record count is less than 24\n",
    "outliers = df_date_counts[df_date_counts['Record_Count'] > 300]\n",
    "print(\"Outliers with MORE than 300 records:\")\n",
    "df_dups = df_raw_daily_solar[df_raw_daily_solar['Date'].isin(outliers['Date'])][['Date','Source_File']].drop_duplicates().sort_values(by='Date')\n",
    "df_dups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f2e76a",
   "metadata": {},
   "source": [
    "#### Load Raw Monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e60baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_dfs = []\n",
    "for file in glob.glob(f'{RAW_DATA_DIR}/solar/Monthly*xls'):\n",
    "    df_header = pd.read_excel(file, nrows=5, header=None)\n",
    "    # get date from header\n",
    "    the_date = None\n",
    "    the_date_label = df_header.iat[0,0]\n",
    "    # format of date is Plant_12-09-2024Chart\n",
    "    result = re.search(r'(\\d{2}-\\d{4})', str(the_date_label))\n",
    "    if result:\n",
    "        date_str = result.group(1)\n",
    "        the_date = datetime.datetime.strptime(date_str, '%m-%Y').date()\n",
    "    df = pd.read_excel(file,skiprows=28)\n",
    "    # add insert date as first column\n",
    "    df['Month'] = the_date\n",
    "    # add file name column in case there are duplicates\n",
    "    df['Source_File'] = os.path.basename(file)\n",
    "    #print(df.head())\n",
    "    # Convert date column to date type format is dd-mm-yyyy\n",
    "    #df['Date'] = pd.to_datetime(df['Date'], format='%d-%m-%Y').dt.date\n",
    "    all_dfs.append(df)\n",
    "# Combine all at once\n",
    "df_raw_monthly_solar = pd.concat(all_dfs, ignore_index=True)\n",
    "# remove all_dfs to free memory\n",
    "del all_dfs\n",
    "# Rename column Time to Date\n",
    "df_raw_monthly_solar = df_raw_monthly_solar.rename(columns={'Time':'Date'})\n",
    "# Convert Date column to date \n",
    "df_raw_monthly_solar['Date'] = pd.to_datetime(df_raw_monthly_solar['Date'], format='%d-%m-%Y').dt.date\n",
    "# Set date as index\n",
    "df_raw_monthly_solar.set_index('Date', inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee72b53",
   "metadata": {},
   "source": [
    "##### Write to CSV and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b009bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing monthly solar data to CSV...\")\n",
    "df_raw_monthly_solar.to_csv(f'{DATA_DIR}/processed_data/df_raw_monthly_solar.csv', index=False)\n",
    "print(f\"Wrote {len(df_raw_monthly_solar)} rows to CSV.\")\n",
    "# Write to SQLite database\n",
    "print(\"Writing monthly solar data to SQLite database...\")\n",
    "row_count = df_raw_monthly_solar.to_sql('weekly_solar_data', con, if_exists='replace', index=True)\n",
    "print(f\"Wrote {row_count} rows to SQLite database.\")\n",
    "# Now do a short query to verify\n",
    "test_query = \"SELECT COUNT(*) FROM weekly_solar_data;\"\n",
    "sys_cur.execute(test_query)\n",
    "result = sys_cur.fetchone()\n",
    "print(f\"Verified {result[0]} rows in SQLite database.\")\n",
    "sqlite3_table_schema(con, 'weekly_solar_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b4721c",
   "metadata": {},
   "source": [
    "__Validate Raw Monthly__\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161e2b98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081da075",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Combined Monthly DataFrame shape:\", df_raw_monthly_solar.shape)  \n",
    "print(\"DataFrame info:\")\n",
    "df_raw_monthly_solar.info()\n",
    "print(\"Columns:\", df_raw_monthly_solar.columns.tolist())\n",
    "print(\"First 5 rows:\")\n",
    "\n",
    "# get min and max Date columns that will indicate range\n",
    "min_date = df_raw_monthly_solar.index.min()\n",
    "max_date = df_raw_monthly_solar.index.max()\n",
    "no_dates = (max_date - min_date).days + 1\n",
    "print(\"No dates in range:\", no_dates)\n",
    "# Date datatype check\n",
    "print(\"Date column datatype:\", df_raw_monthly_solar.index.dtype)\n",
    "print(f\"Date range: {min_date} to {max_date}\")\n",
    "#no_of_days = (max_date - min_date).days + 1\n",
    "#print(f\"Total number of days in range: {no_of_days}\")\n",
    "# Count unique dates\n",
    "unique_dates = df_raw_monthly_solar.index.nunique()\n",
    "print(f\"Unique dates count: {unique_dates}\")\n",
    "if unique_dates == no_dates:\n",
    "    print(\"No missing dates in the monthly solar data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f286fb",
   "metadata": {},
   "source": [
    "__Plot a few things to see how the data looks__\n",
    "\n",
    " Date                \n",
    " Yield(kWh)                 \n",
    " Full Load Hours(h)  \n",
    " Charged(kWh)        \n",
    " Discharged(kWh)     \n",
    " Exported(kWh)       \n",
    " Imported(kWh)       \n",
    " Net Import(kWh)     \n",
    " Load(kWh)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd0042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot date vs all the other measures in kWh on one plot\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Set title\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Monthly Solar Data Measures over Time')\n",
    "for column in df_raw_monthly_solar.columns:\n",
    "    if column not in ['Date', 'Source_File', 'Month','Number','Earnings(EUR)','Net Import(kWh)']:\n",
    "        sns.lineplot(data=df_raw_monthly_solar, x='Date', y=column, label=column)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7204e8fd",
   "metadata": {},
   "source": [
    "#### Load Yearly Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802832d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_dfs = []\n",
    "for file in glob.glob(f'{RAW_DATA_DIR}/solar/Annual*xls'):\n",
    "    df = pd.read_excel(file,skiprows=28)\n",
    "    # add insert date as first column\n",
    "    df.insert(0, 'Date', the_date)\n",
    "    # The total yield as last column\n",
    "    df['Total_Yield(kWh)'] = total_yield\n",
    "    # add file name column in case there are duplicates\n",
    "    df['Source_File'] = os.path.basename(file)\n",
    "    #print(df.head())\n",
    "    all_dfs.append(df)\n",
    "# Combine all at once\n",
    "df_raw_annual_solar = pd.concat(all_dfs, ignore_index=True)\n",
    "# remove all_dfs to free memory\n",
    "del all_dfs\n",
    "# Rename column Time to Date\n",
    "df_raw_annual_solar = df_raw_annual_solar.rename(columns={'Time':'Month'})\n",
    "# Create Date column from Month\n",
    "df_raw_annual_solar['Date'] = '01-' + df_raw_annual_solar['Month']\n",
    "# Convert Date column to datetime\n",
    "df_raw_annual_solar['Date'] = pd.to_datetime(df_raw_annual_solar['Date'], format='%d-%m-%Y').dt.date\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdf50d5",
   "metadata": {},
   "source": [
    "__Validate Yearly__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3f259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combined Annual DataFrame shape:\", df_raw_annual_solar.shape)  \n",
    "print(\"DataFrame info:\")\n",
    "df_raw_annual_solar.info()\n",
    "print(\"Columns:\", df_raw_annual_solar.columns.tolist())\n",
    "df_raw_annual_solar.head(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01840c",
   "metadata": {},
   "source": [
    "__Plot Data__\n",
    "\n",
    "Plot to see what it looks like. Should see a bell like curve because of the seasonality\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de8b229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot date vs all the other measures in kWh on one plot\n",
    "import matplotlib.dates as mdates\n",
    "sns.set(style=\"whitegrid\")\n",
    "# Set title\n",
    "plt.figure(figsize=(14, 8))\n",
    "plt.title('Annual Solar Data Measures over Time')\n",
    "for column in df_raw_annual_solar.columns:\n",
    "    if column not in ['Date', 'Source_File', 'Month','Number','Earnings(EUR)','Net Import(kWh)']:\n",
    "        sns.lineplot(data=df_raw_annual_solar, x='Date', y=column, label=column)\n",
    "ax = plt.gca() # Get current axis\n",
    "\n",
    "# 1. Set the Locator: Tell matplotlib to put a tick at every month\n",
    "ax.xaxis.set_major_locator(mdates.MonthLocator(interval=1))\n",
    "\n",
    "# 2. Set the Formatter: Tell matplotlib how to write the text (e.g., Jan-2023)\n",
    "ax.xaxis.set_major_formatter(mdates.DateFormatter('%b-%Y'))\n",
    "\n",
    "# 3. Rotate ticks\n",
    "plt.tick_params(axis='x', rotation=90)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d243b",
   "metadata": {},
   "source": [
    "##### Write to CSV and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1480b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing yearly solar data to CSV...\")\n",
    "df_raw_annual_solar.to_csv(f'{DATA_DIR}/processed_data/df_raw_yearly_solar.csv', index=False)\n",
    "print(f\"Wrote {len(df_raw_annual_solar)} rows to CSV.\")\n",
    "# Write to SQLite database\n",
    "print(\"Writing yearly solar data to SQLite database...\")\n",
    "row_count = df_raw_annual_solar.to_sql('monthly_solar_data', con, if_exists='replace', index=True)\n",
    "print(f\"Wrote {row_count} rows to SQLite database.\")\n",
    "# Now do a short query to verify\n",
    "test_query = \"SELECT COUNT(*) FROM monthly_solar_data;\"\n",
    "sys_cur.execute(test_query)\n",
    "result = sys_cur.fetchone()\n",
    "print(f\"Verified {result[0]} rows in SQLite database.\")\n",
    "sqlite3_table_schema(con, 'monthly_solar_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b011745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all tables in the database\n",
    "sys_cur.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "tables = sys_cur.fetchall()\n",
    "print(\"Tables in the database:\")\n",
    "for table in tables:\n",
    "    print(table[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dc9094",
   "metadata": {},
   "source": [
    "### Load ESB Microgeneration Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be11074a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load into pandas dataframe csv file\n",
    "df_raw_esb_microgen = pd.read_csv(f'{RAW_DATA_DIR}/esb/BGE_Export_HDF_18_11_2025.csv')\n",
    "# change Date column to date\n",
    "df_raw_esb_microgen['Date'] = pd.to_datetime(df_raw_esb_microgen['Date']).dt.date \n",
    "# change Date to index\n",
    "df_raw_esb_microgen.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b352c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ESB Microgen DataFrame shape:\", df_raw_esb_microgen.shape)\n",
    "print(\"DataFrame info:\")\n",
    "df_raw_esb_microgen.info()\n",
    "print(\"Columns:\", df_raw_esb_microgen.columns.tolist())\n",
    "print(\"First 5 rows:\")\n",
    "print(df_raw_esb_microgen.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57953167",
   "metadata": {},
   "source": [
    "#### Write to CSV and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce8082a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing daily ESB microgen data to CSV...\")\n",
    "df_raw_esb_microgen.to_csv(f'{DATA_DIR}/processed_data/df_raw_daily_esb_microgen.csv', index=False)\n",
    "print(f\"Wrote {len(df_raw_esb_microgen)} rows to CSV.\")\n",
    "# Write to SQLite database\n",
    "print(\"Writing daily ESB microgen data to SQLite database...\")\n",
    "row_count = df_raw_esb_microgen.to_sql('daily_esb_microgen_data', con, if_exists='replace', index=True)\n",
    "print(f\"Wrote {row_count} rows to SQLite database.\")\n",
    "# Now do a short query to verify\n",
    "test_query = \"SELECT COUNT(*) FROM daily_esb_microgen_data;\"\n",
    "sys_cur.execute(test_query)\n",
    "result = sys_cur.fetchone()\n",
    "print(f\"Verified {result[0]} rows in SQLite database.\")\n",
    "sqlite3_table_schema(con, 'daily_esb_microgen_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a0d3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot esb microgen data\n",
    "plt.figure(figsize=(12,6))\n",
    "ss = df_raw_esb_microgen['Export Volume (kWh)']\n",
    "ss.plot()\n",
    "# plot a smooth line\n",
    "ss.rolling(window=30,min_periods=1).mean().plot()\n",
    "plt.title('ESB Microgen Power Output Over Time')\n",
    "plt.xlabel('Date')\n",
    "# set x axis major ticks to monthly at 90 degrees\n",
    "plt.gca().xaxis.set_major_locator(plt.matplotlib.dates.MonthLocator())\n",
    "plt.gca().xaxis.set_major_formatter(plt.matplotlib.dates.DateFormatter('%Y-%m-%d'))\n",
    "plt.gcf().autofmt_xdate(rotation=90)\n",
    "plt.ylabel('Power (kW)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e17878f",
   "metadata": {},
   "source": [
    "__Analysis__\n",
    "\n",
    "The question is , does ESB microgeneration kWH per day compare with the amount of power exported to the grid. The data from solar system (df_raw_monthly_solar) Column Exported(kWh)  and ESB microgeneration(df_raw_esb_microgen) column (Export Volume (kWh)) need to be joined on the same date grain. That is date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77670bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_esb_solar_merged = pd.merge(left=df_raw_esb_microgen['Export Volume (kWh)'].copy().reset_index('Date'),\n",
    "         right=df_raw_monthly_solar['Exported(kWh)'].copy().reset_index('Date'),how='inner', on='Date')\n",
    "# subtact the two columns to see difference\n",
    "df_esb_solar_merged['Difference(kWh)'] = df_esb_solar_merged['Exported(kWh)'] - df_esb_solar_merged['Export Volume (kWh)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0a675e",
   "metadata": {},
   "source": [
    "__Plot Results__\n",
    "\n",
    "This will compare the results of data measured by ESB and Solar System Inverter. This is to see if there are any obvious differences between the measurements. There is expected to be a slight difference because the two devices are different , and they are measureing at different point. At a visual difference there is no real measurable difference except for 1 day , which may be related to a power outage . \n",
    "\n",
    "__Observation__\n",
    "\n",
    "The graph show that there is no material difference , if the outlier is removed that it shows a over reporting for ESB , or under for solar . We cannot determine which one is correct , rather we know either , or both are slighly incorrect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b2933",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(5, 1, figsize=(12, 8))\n",
    "\n",
    "# Combined plot first\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Export Volume (kWh)', label='ESB Export Volume (kWh)', ax=ax[0])\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Exported(kWh)', label='Solar Exported (kWh)', ax=ax[0])\n",
    "ax[0].set_xlabel('')\n",
    "\n",
    "# Plot each separately\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Export Volume (kWh)', label='ESB Export Volume (kWh)', ax=ax[1])\n",
    "ax[1].set_xlabel('')\n",
    "\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Exported(kWh)', label='Solar Exported (kWh)', ax=ax[2])\n",
    "ax[2].set_xlabel('')\n",
    "\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Difference(kWh)', label='Difference (kWh)', ax=ax[3])\n",
    "ax[3].set_xlabel('')\n",
    "\n",
    "# Replace outlier values in Difference(kWh) with NaN for better visualization\n",
    "# using a lambda function to set values greater than abs(5) to NaN\n",
    "# a lamba is an anonymous function in Python\n",
    "df_esb_solar_merged['Difference_Cleaned(kWh)'] = df_esb_solar_merged['Difference(kWh)'].apply(lambda x: x if abs(x) < 5 else np.nan)\n",
    "sns.lineplot(data=df_esb_solar_merged, x='Date', y='Difference_Cleaned(kWh)', label='Difference Cleaned (kWh)', ax=ax[4])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d9c523",
   "metadata": {},
   "source": [
    "__Method Comparison Study__\n",
    "\n",
    "I used the following prompt on gemini 3.0 Thinking\n",
    "\n",
    "I am doing a project on my solar panels at home , i have two measures of the same thing. Exported KWh but from different sources , what statistical techinques can i use to analise the difference\n",
    "\n",
    "The prompt gave me a number of options\n",
    "\n",
    "1. The Gold Standard: Bland-Altman Plot\n",
    "2. Hypothesis Testing: Paired t-Test\n",
    "3. Error Metrics: MAE and RMSE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ebad3",
   "metadata": {},
   "source": [
    "__Bland-Altman Plot__\n",
    "\n",
    "This is the primary visual test used in science and engineering to compare two measurement techniques.\n",
    "\n",
    "What it does: It plots the Difference between the two sources (Y-axis) against the Average of the two sources (X-axis).\n",
    "\n",
    "Why use it: It instantly reveals if the error gets worse as generation gets higher (proportional bias) or if one meter always reads higher by a fixed amount (systematic bias).\n",
    "\n",
    "Examing the results\n",
    "\n",
    "- there is one outlier\n",
    "- systemic bias - there is a slight bias above the meaning that one of the devices is overreading, or alternatively one of the readers is slightly higher than the other , we do not know which one is the truth\n",
    "- consistency - there is strong consistency because the reading consistent along the limits of agreement\n",
    "\n",
    "[Wiki Band Altman Plot](https://en.wikipedia.org/wiki/Bland%E2%80%93Altman_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View Bland-Altman Plot\n",
    "# asarray converts pandas series to numpy array\n",
    "data1 = np.asarray(df_esb_solar_merged['Export Volume (kWh)'])\n",
    "data2 = np.asarray(df_esb_solar_merged['Exported(kWh)'])\n",
    "# calculate the mean between the two measurements and the difference\n",
    "mean = np.mean([data1, data2], axis=0)\n",
    "diff = data1 - data2\n",
    "plt.scatter(mean, diff)\n",
    "plt.axhline(np.mean(diff), color='orange', linestyle='--')\n",
    "plt.axhline(np.mean(diff) + 1.96*np.std(diff), color='red', linestyle='--')\n",
    "plt.axhline(np.mean(diff) - 1.96*np.std(diff), color='red', linestyle='--')\n",
    "plt.xlabel('Mean of Two Measurements')\n",
    "plt.ylabel('Difference Between Measurements')\n",
    "plt.title('Bland-Altman Plot')\n",
    "# The yellow line represents the mean difference\n",
    "# The red lines represent the limits of agreement (mean difference ± 1.96 standard deviations)\n",
    "plt.legend(['Mean Difference', 'Limits of Agreement'])\n",
    "plt.show()\n",
    "# print out the outlier\n",
    "\n",
    "print(\"Outliers where difference is greater than 5 kWh:\")\n",
    "outliers = df_esb_solar_merged[abs(df_esb_solar_merged['Difference(kWh)']) > 5]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7011c438",
   "metadata": {},
   "source": [
    "__Hypothesis Test ( Paired t-test )__\n",
    "\n",
    "Paired t-test: Use this if your differences are normally distributed (bell curve)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa5c57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# remove the outliers for the t-test\n",
    "df_esb_solar_merged_no_outliers = df_esb_solar_merged[abs(df_esb_solar_merged['Difference(kWh)']) <= 5]\n",
    "data1 = np.asarray(df_esb_solar_merged_no_outliers['Export Volume (kWh)'])\n",
    "data2 = np.asarray(df_esb_solar_merged_no_outliers['Exported(kWh)'])\n",
    "# perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(data1, data2)\n",
    "print(f\"\\n--- Statistical Test (Paired t-test) ---\")\n",
    "print(f\"P-value: {p_value:.5f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: Statistically Significant Difference (The sources are NOT the same)\")\n",
    "else:\n",
    "    print(\"Result: No significant difference found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745ed8f",
   "metadata": {},
   "source": [
    "You can know if the paired t-test is valid by checking three specific assumptions.\n",
    "\n",
    "For solar data, the most critical one is Normality of the Differences. Many people mistakenly think the solar generation itself needs to be a Bell curve (which it never is—it's zero at night and high in summer). This is wrong. Only the difference between your Inverter and Utility meter needs to look like a Bell curve.\n",
    "\n",
    "The 3 Assumptions Checklist\n",
    "Continuous Data: Verified. (Kilowatt-hours are continuous numbers).\n",
    "\n",
    "Independence of Pairs: Verified. (The error on Monday doesn't \"cause\" the error on Tuesday; each day is a separate event).\n",
    "\n",
    "Normality of Differences: Must be tested. The Inverter - Utility values should form a Bell curve (Normal Distribution).\n",
    "\n",
    "How to Interpret the Results\n",
    "If the Histogram is a Bell Curve (and p-value > 0.05): Your differences are random \"noise.\" The Paired t-test is the correct tool.\n",
    "\n",
    "If the Histogram is skewed or flat (and p-value < 0.05): Your differences are not random. This often happens in solar if the inverter has a \"cutoff\" issue or if that outlier you found earlier is skewing the data.\n",
    "\n",
    "Solution: Use the Wilcoxon Signed-Rank Test. It does the same job as the t-test but doesn't care about Bell curves.\n",
    "\n",
    "Python code: stats.wilcoxon(df['Inverter'], df['Meter'])\n",
    "\n",
    "Important Note on Your Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86e0f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "df = df_esb_solar_merged_no_outliers.copy()\n",
    "df = df.rename(columns={'Export Volume (kWh)': 'Meter', 'Exported(kWh)': 'Inverter'})\n",
    "# 1. Calculate the Difference\n",
    "df['Diff'] = df['Inverter'] - df['Meter']\n",
    "\n",
    "# 2. Visual Check: Histogram\n",
    "# It should look like a Bell Curve centered roughly around 0 (or your bias)\n",
    "plt.hist(df['Diff'], bins=10, edgecolor='black')\n",
    "plt.title('Distribution of Differences (Inverter - Meter)')\n",
    "plt.xlabel('Difference (kWh)')\n",
    "plt.show()\n",
    "\n",
    "# 3. Visual Check: Q-Q Plot\n",
    "# The dots should roughly follow the red line\n",
    "stats.probplot(df['Diff'], dist=\"norm\", plot=plt)\n",
    "plt.title(\"Q-Q Plot\")\n",
    "plt.show()\n",
    "\n",
    "# 4. Mathematical Check: Shapiro-Wilk Test\n",
    "# This gives you a p-value.\n",
    "stat, p_value = stats.shapiro(df['Diff'])\n",
    "\n",
    "print(f\"Shapiro-Wilk P-Value: {p_value:.5f}\")\n",
    "\n",
    "if p_value > 0.05:\n",
    "    print(\"PASSED: Data is Normal. You CAN use the Paired t-test.\")\n",
    "else:\n",
    "    print(\"FAILED: Data is NOT Normal. Use 'Wilcoxon Signed-Rank Test' instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c7a44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.wilcoxon(df['Inverter'], df['Meter'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a47b504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Run the Wilcoxon Signed-Rank Test\n",
    "# We use 'two-sided' to check if they are simply different (not specifically higher/lower)\n",
    "statistic, p_value = stats.wilcoxon(df['Inverter'], df['Meter'], alternative='two-sided')\n",
    "\n",
    "print(\"--- Wilcoxon Signed-Rank Test Results ---\")\n",
    "print(f\"Statistic: {statistic}\")\n",
    "print(f\"P-value: {p_value:.5f}\")\n",
    "\n",
    "# 3. Interpretation Logic\n",
    "print(\"\\n--- Conclusion ---\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Result: SIGNIFICANT DIFFERENCE\")\n",
    "    print(\"There is a statistically significant difference between the Inverter and Utility meter.\")\n",
    "    \n",
    "    # Check who is higher by looking at the median difference\n",
    "    median_diff = (df['Inverter'] - df['Meter']).median()\n",
    "    if median_diff > 0:\n",
    "        print(f\"Direction: The Inverter reads higher (Median diff: {median_diff:.3f} kWh)\")\n",
    "    else:\n",
    "        print(f\"Direction: The Utility Meter reads higher (Median diff: {median_diff:.3f} kWh)\")\n",
    "\n",
    "else:\n",
    "    print(\"Result: NO SIGNIFICANT DIFFERENCE\")\n",
    "    print(\"The differences are likely just random noise. The devices measure effectively the same.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658e25b6",
   "metadata": {},
   "source": [
    "Plot of wilcox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a15ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Create the Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# --- Plot A: Histogram (Distribution Check) ---\n",
    "sns.histplot(df['Diff'], kde=True, ax=axes[0], color='skyblue')\n",
    "axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero Difference')\n",
    "axes[0].set_title('Histogram of Differences')\n",
    "axes[0].set_xlabel('Difference (kWh)')\n",
    "axes[0].legend()\n",
    "\n",
    "# --- Plot B: Boxplot (The Wilcoxon Companion) ---\n",
    "sns.boxplot(y=df['Diff'], ax=axes[1], color='lightgreen')\n",
    "axes[1].axhline(0, color='red', linestyle='--', linewidth=2, label='Zero Difference')\n",
    "axes[1].set_title('Boxplot of Differences')\n",
    "axes[1].set_ylabel('Difference (kWh)')\n",
    "axes[1].legend()\n",
    "\n",
    "# Add a note about the median\n",
    "median_val = df['Diff'].median()\n",
    "axes[1].text(0.1, median_val, f' Median: {median_val:.2f}', \n",
    "             color='black', weight='bold', ha='left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2b8d38",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41986554",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7b094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Assuming 'df' is your dataframe and 'Diff' is the column plotted above\n",
    "# 1. Calculate the Systematic Bias (using Median to be robust against outliers)\n",
    "bias = df['Diff'].median()\n",
    "\n",
    "print(f\"Detected Systematic Bias: {bias:.4f} kWh\")\n",
    "\n",
    "# 2. Create the \"Normalized\" Data (Centralized on Zero)\n",
    "# We subtract the bias from the difference\n",
    "df['Diff_Normalized'] = df['Diff'] - bias\n",
    "\n",
    "# --- PLOTTING ---\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot Original (with Bias)\n",
    "sns.histplot(df['Diff'], color='skyblue', alpha=0.5, label='Original (Biased)', kde=True)\n",
    "\n",
    "# Plot Normalized (Centered on Zero)\n",
    "sns.histplot(df['Diff_Normalized'], color='green', alpha=0.5, label='Normalized (Bias Removed)', kde=True)\n",
    "\n",
    "# Add the Zero line\n",
    "plt.axvline(0, color='red', linestyle='--', linewidth=2, label='Zero (Perfect Agreement)')\n",
    "\n",
    "plt.title('Effect of Bias Correction (Zero-Centering)')\n",
    "plt.xlabel('Difference (kWh)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# --- CHECK NEW METRICS ---\n",
    "new_median = df['Diff_Normalized'].median()\n",
    "print(f\"New Median after correction: {new_median:.4f} kWh (Should be 0.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56704009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- METHOD A: Cohen's d (For Paired t-test) ---\n",
    "# Mean of difference / Std Dev of difference\n",
    "cohens_d = np.mean(diff) / np.std(diff, ddof=1)\n",
    "\n",
    "# --- METHOD B: Rank-Biserial Correlation (For Wilcoxon) ---\n",
    "# We perform the Wilcoxon test to get the rank sums\n",
    "# exact=True is preferred for small datasets (<25), False for larger\n",
    "w_stat, _ = stats.wilcoxon(df['Inverter'], df['Meter'])\n",
    "\n",
    "# Calculate total ranks (Formula: N * (N + 1) / 2)\n",
    "n = len(diff)\n",
    "total_rank_sum = n * (n + 1) / 2\n",
    "\n",
    "# In scipy, w_stat is the minimum of the two sums (positive or negative)\n",
    "# So we can reverse-engineer the correlation (r)\n",
    "# r = 1 - (2 * w_stat) / total_rank_sum\n",
    "r_biserial = 1 - (2 * w_stat) / total_rank_sum\n",
    "\n",
    "print(\"--- Effect Size Results ---\")\n",
    "\n",
    "print(f\"\\n1. Cohen's d (If using t-test): {cohens_d:.3f}\")\n",
    "# Interpretation Logic for Cohen's d\n",
    "if abs(cohens_d) < 0.2: print(\"   -> Negligible Effect\")\n",
    "elif abs(cohens_d) < 0.5: print(\"   -> Small Effect\")\n",
    "elif abs(cohens_d) < 0.8: print(\"   -> Medium Effect\")\n",
    "else: print(\"   -> Large Effect (The difference is very noticeable)\")\n",
    "\n",
    "print(f\"\\n2. Rank-Biserial r (If using Wilcoxon): {r_biserial:.3f}\")\n",
    "# Interpretation Logic for r\n",
    "if abs(r_biserial) < 0.1: print(\"   -> Negligible Effect\")\n",
    "elif abs(r_biserial) < 0.3: print(\"   -> Small Effect\")\n",
    "elif abs(r_biserial) < 0.5: print(\"   -> Medium Effect\")\n",
    "else: print(\"   -> Large Effect\")\n",
    "\n",
    "print(\"\\n------------------------------------------------\")\n",
    "print(\"Note: If the number is POSITIVE, the Inverter is higher.\")\n",
    "print(\"Note: If the number is NEGATIVE, the Meter is higher.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca351bc8",
   "metadata": {},
   "source": [
    "### Load  Weather Data\n",
    "\n",
    "The first iteration the local weather was downloaded from Data.gov.ie - Dublin Airport Hourly Data. After some investigation , a more general source was found Meteostat was found. This gives a similar data set to data.gov.ie with the advantage that it downloads directly into a dataframe , and it has a local cache.\n",
    "\n",
    "[Meteostat](https://meteostat.net/en/)  \n",
    "[Meteostat Columns](https://dev.meteostat.net/formats.html#time-format)  \n",
    "[Dublin Airport Hourly Data](https://data.gov.ie/dataset/dublin-airport-hourly-data)  \n",
    "[Dublin Airport Weather](https://cli.fusio.net/cli/climate_data/webdata/hly532.csv)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016f55a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load hourly weather data for Dublin\n",
    "#df_weather_dublin = pd.read_csv('https://cli.fusio.net/cli/climate_data/webdata/hly532.csv', skiprows=23)\n",
    "mt.Stations.cache_dir = METEOSTAT_CACHE_DIR\n",
    "stations = mt.Stations()\n",
    "# Get the stations near the specified latitude and longitude\n",
    "stations = stations.nearby(LATITUDE, LONGITUDE)\n",
    "# Get the first station from the list , which is usually the closest one\n",
    "df_station = stations.fetch(1)\n",
    "# Reset index to access station ID\n",
    "df_station.reset_index(inplace=True)\n",
    "print(f\"Station ID: {df_station['id'][0]}\")\n",
    "print(f\"Station Name: {df_station['name'][0]}\")\n",
    "print(f\"Station Distance: {df_station['distance'][0]:.2f} m\")\n",
    "# Get local station ID\n",
    "station_id = df_station['id'][0]\n",
    "# Get hourly data\n",
    "data = mt.Hourly(station_id, WEATHER_START_DATE, WEATHER_END_DATE,timezone='Europe/Dublin')\n",
    "# fetch the data\n",
    "weather_df = data.fetch()\n",
    "# set index to default integer index\n",
    "weather_df.reset_index(inplace=True)\n",
    "# find all missing values in the dataframe\n",
    "missing_values = weather_df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0].to_string())\n",
    "# fix missing values by forward filling and then backward filling\n",
    "weather_df.ffill(inplace=True)\n",
    "weather_df.bfill(inplace=True)\n",
    "# find all missing values in the dataframe\n",
    "missing_values = weather_df.isnull().sum()\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values[missing_values > 0].to_string())\n",
    "# explode the dataframe time to 10 minute intervals\n",
    "weather_df.set_index('time', inplace=True)\n",
    "weather_df = weather_df.resample('10T').interpolate(method='linear')\n",
    "weather_df.reset_index(inplace=True)\n",
    "# we are only interested in time, temperature, humidity, and solar radiation\n",
    "weather_df = weather_df[['time', 'temp', 'rhum', 'tsun','coco']]\n",
    "# rename columns for clarity\n",
    "weather_df.rename(columns={'time': 'DateTime', 'temp': 'Temperature(C)', 'rhum': 'Humidity(%)', 'tsun': 'Solar Radiation(W/m^2)','coco': 'Condition Code'}, inplace=True)\n",
    "# Change DateTime - Drop timezone info to Europe/Dublin need that later for merging with other data\n",
    "weather_df['DateTime'] = weather_df['DateTime'].dt.tz_convert('Europe/Dublin').dt.tz_localize(None)\n",
    "print(\"Weather DataFrame shape:\", weather_df.shape)\n",
    "print(\"DataFrame info:\")\n",
    "weather_df.info()\n",
    "print(\"Columns:\", weather_df.columns.tolist())\n",
    "print(\"First 5 rows:\")\n",
    "print(weather_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e572ea",
   "metadata": {},
   "source": [
    "#### Write to CSV and Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e07b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Writing daily Dublin weather data to CSV...\")\n",
    "weather_df.to_csv(f'{DATA_DIR}/processed_data/df_weather.csv', index=False)\n",
    "print(f\"Wrote {len(weather_df)} rows to CSV.\")\n",
    "# Write to SQLite database\n",
    "print(\"Writing daily Dublin weather data to SQLite database...\")\n",
    "row_count = weather_df.to_sql('hourly_weather_data', con, if_exists='replace', index=True)\n",
    "print(f\"Wrote {row_count} rows to SQLite database.\")\n",
    "# Now do a short query to verify\n",
    "test_query = \"SELECT COUNT(*) FROM hourly_weather_data;\"\n",
    "sys_cur.execute(test_query)\n",
    "result = sys_cur.fetchone()\n",
    "print(f\"Verified {result[0]} rows in SQLite database.\")\n",
    "sqlite3_table_schema(con, 'hourly_weather_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ce9652",
   "metadata": {},
   "source": [
    "#### Validate Data\n",
    "\n",
    "Plot the temperature , humidity and solar radiation over time. There are 10K rows , thus it takes some time.\n",
    "Notice how irish weather is inconsistent. This makes trying to find predict solar output from weather a challenge.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f89627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "fig,ax = plt.subplots(4,1,figsize=(12,8))\n",
    "\n",
    "fig.suptitle('Weather Data Over Time')\n",
    "\n",
    "sns.lineplot(data=weather_df, x='DateTime', y='Temperature(C)', ax=ax[0], color='red')\n",
    "ax[0].set_title('Temperature Over Time')\n",
    "sns.lineplot(data=weather_df, x='DateTime', y='Humidity(%)', ax=ax[1], color='blue')\n",
    "ax[1].set_title('Humidity Over Time')\n",
    "sns.lineplot(data=weather_df, x='DateTime', y='Solar Radiation(W/m^2)', ax=ax[2], color='orange')\n",
    "ax[2].set_title('Solar Radiation Over Time')\n",
    "sns.lineplot(data=weather_df, x='DateTime', y='Condition Code', ax=ax[3], color='green')\n",
    "ax[3].set_title('Condition Code Over Time')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c09086",
   "metadata": {},
   "source": [
    "## Examining Inverter Export Measurement and Utility Import Measurement\n",
    "\n",
    "__Introduction__\n",
    "\n",
    "The Solor system measures the output of exported to the grid power (KWh) ,and the Utility ( ESB ) measures the micro generation import (KWh). In this section the two will be compared. Initially this started as an audit of the measurement , _am i being ripped off_ , then once that was disproved , the comparison of two assay methods against the same sample. This was more interesteresting\n",
    "\n",
    "![House/ESB](images/ESB_SOLAR.drawio.png)\n",
    "\n",
    "- **Output measurement:**\n",
    "  - Source: Monthly Export From Inverter\n",
    "  - DataFrame: `df_raw_monthly_solar`\n",
    "- **Input measurement:**\n",
    "  - Source: ESB Microgeneration\n",
    "  - DataFrame: `df_raw_esb_microgen`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2effaa",
   "metadata": {},
   "source": [
    "## Predicting Solar Output from Weather\n",
    "\n",
    "The object of this part of the project is to use numerical methods to predict the solar panel output based on weather infomation.\n",
    "\n",
    "There are two sources of data , the solar system power generation rate , and the weather data which includes solar radiation , temperature and humidity. \n",
    "\n",
    "Various methods will be tried to compare there accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df1034b",
   "metadata": {},
   "source": [
    "### Data Preperation\n",
    "\n",
    "Both sources of data have already been loaded into data frames. \n",
    "\n",
    "__Solar Data : df_raw_daily_solar__\n",
    "\n",
    "This has a grain of 5 minute intervals with measure for PV(W) - Solar Power\n",
    "\n",
    "__ Weather : df_weather__\n",
    "\n",
    "This has a grain of 10 minute intervals with measures for Temperature , Humidity and Solar Radiance.\n",
    "\n",
    "The following will be done\n",
    "\n",
    "1. Both data sets will be converted to the same grain ( 10 minute intervals) . \n",
    "2. The data sets will be merged into 1 data set\n",
    "3. The data set will be normalized using the min max approach\n",
    "4. The date time will be normalized using a data cyclic approach ( sine ) with the centre on the summer soltice\n",
    "\n",
    "[Pandas : Resample](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.resample.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fa430b",
   "metadata": {},
   "source": [
    "__Solar__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972f25e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the columns DateTime, PV(W) and only get times every 10 minutes\n",
    "df_solar_10min = df_raw_daily_solar[['DateTime', 'PV(W)']].copy()\n",
    "# filter to only keep rows where minute is multiple of 10 - because it is at 5 minute intervals\n",
    "df_solar_10min = df_solar_10min[df_solar_10min['DateTime'].dt.minute % 10 == 0]\n",
    "df_solar_10min.set_index('DateTime', inplace=True)\n",
    "df_solar_10min.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c7461a",
   "metadata": {},
   "source": [
    "__Weather__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852d0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weather data also at 10 minute intervals so set index to DateTime\n",
    "df_weather_10min = weather_df.copy()\n",
    "# sort index\n",
    "df_weather_10min.sort_index(inplace=True)\n",
    "print(\"df_wather_10min shape:\", df_weather_10min.shape)\n",
    "print(\"df_solar_10min info:\")\n",
    "df_solar_10min.info()\n",
    "print(\"df_weather_10min info:\")\n",
    "df_solar_10min.info()\n",
    "# set index to DateTime\n",
    "df_weather_10min.set_index('DateTime', inplace=True)\n",
    "# Sort index\n",
    "df_weather_10min.sort_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5581601a",
   "metadata": {},
   "source": [
    "__Merge__\n",
    "\n",
    "[merge_asof](https://pandas.pydata.org/docs/reference/api/pandas.merge_asof.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f596a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge asof to get nearest time match\n",
    "df_merged_10min = pd.merge_asof(left = df_solar_10min,\n",
    "                                right = df_weather_10min, left_index=True, right_index=True, direction='nearest')\n",
    "print(\"Merged 10-minute DataFrame shape:\", df_merged_10min.shape)\n",
    "# Now add datetime derived columns for analysis\n",
    "# All centered around summer soltstice June 21 and noon , normalized between 0 and 1 using sin function\n",
    "# Month of year, day of year, hour of day\n",
    "df_merged_10min['Month_Sin'] = np.sin(2 * np.pi * (df_merged_10min.index.month - 1) / 12)\n",
    "df_merged_10min['DayOfYear_Sin'] = np.sin(2 * np.pi * (df_merged_10min.index.dayofyear - 1) / 365)\n",
    "df_merged_10min['HourOfDay_Sin'] = np.sin(2 * np.pi * (df_merged_10min.index.hour) / 24)\n",
    "print(\"Final Merged DataFrame shape:\", df_merged_10min.shape)\n",
    "df_merged_10min.reset_index(inplace=True)\n",
    "print(\"DataFrame info:\")\n",
    "df_merged_10min.info()\n",
    "print(\"Columns:\", df_merged_10min.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d9df95",
   "metadata": {},
   "source": [
    "__Histograms of each of the measures__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f211d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of of all the measures/features\n",
    "fig, ax = plt.subplots(2,3, figsize=(12,8))\n",
    "fig.suptitle('Histograms of Merged 10-Minute Data Features')\n",
    "sns.histplot(data=df_merged_10min, x='PV(W)', bins=50, kde=True, ax=ax[0,0], color='green')\n",
    "ax[0,0].set_title('Histogram of PV(W)')\n",
    "sns.histplot(data=df_merged_10min, x='Temperature(C)', bins=50, kde=True, ax=ax[0,1], color='red')\n",
    "ax[0,1].set_title('Histogram of Temperature(C)')\n",
    "sns.histplot(data=df_merged_10min, x='Humidity(%)', bins=50, kde=True, ax=ax[1,0], color='blue')\n",
    "ax[1,0].set_title('Histogram of Humidity(%)')\n",
    "sns.histplot(data=df_merged_10min, x='Solar Radiation(W/m^2)', bins=50, kde=True, ax=ax[1,1], color='orange')\n",
    "ax[1,1].set_title('Histogram of Solar Radiation(W/m^2)')\n",
    "sns.histplot(data=df_merged_10min, x='Condition Code', bins=50, kde=True, ax=ax[1,2], color='purple')\n",
    "ax[1,2].set_title('Histogram of Condition Code')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff83d92a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918c69a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a pairplot to see relationships between features\n",
    "sns.pairplot(df_merged_10min[['PV(W)', 'Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)','Condition Code']])\n",
    "plt.suptitle('Pairplot of Merged 10-Minute Data Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450606d5",
   "metadata": {},
   "source": [
    "_Normalization_\n",
    "\n",
    "The Min Max Scaler is used from scipy. This will convert each of the features or measures to a value between 0 -  1. This helps in traing. If the data is not scaled then there is a risk that one measure will override the others because of it's large relative magitude.\n",
    "\n",
    "(MinMaxScaler)[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html] \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c1a53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data using Min-Max scaling\n",
    "# check if scikit-learn is installed , only needed in Jupyter notebooks\n",
    "#!pip install scikit-learn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_merged_10min_normalized = scaler.fit_transform(df_merged_10min[['PV(W)', 'Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']])\n",
    "df_merged_10min_normalized = pd.DataFrame(df_merged_10min_normalized, columns=['PV(W)', 'Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin'])\n",
    "print(\"Normalized 10-minute DataFrame shape:\", df_merged_10min_normalized.shape)\n",
    "print(\"DataFrame info:\")\n",
    "df_merged_10min_normalized.info()\n",
    "print(\"Columns:\", df_merged_10min_normalized.columns.tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d83b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram of of all the measures/features\n",
    "fig, ax = plt.subplots(2,4, figsize=(12,8))\n",
    "fig.suptitle('Histograms of Merged 10-Minute Data Features')\n",
    "sns.histplot(data=df_merged_10min, x='PV(W)', bins=50, kde=True, ax=ax[0,0], color='green')\n",
    "ax[0,0].set_title('Histogram of PV(W)')\n",
    "sns.histplot(data=df_merged_10min, x='Temperature(C)', bins=50, kde=True, ax=ax[0,1], color='red')\n",
    "ax[0,1].set_title('Histogram of Temperature(C)')\n",
    "sns.histplot(data=df_merged_10min, x='Humidity(%)', bins=50, kde=True, ax=ax[1,0], color='blue')\n",
    "ax[1,0].set_title('Histogram of Humidity(%)')\n",
    "sns.histplot(data=df_merged_10min, x='Solar Radiation(W/m^2)', bins=50, kde=True, ax=ax[1,1], color='orange')\n",
    "ax[1,1].set_title('Histogram of Solar Radiation(W/m^2)')\n",
    "sns.histplot(data=df_merged_10min, x='Condition Code', bins=50, kde=True, ax=ax[1,2], color='purple')\n",
    "ax[1,2].set_title('Histogram of Condition Code')\n",
    "sns.histplot(data=df_merged_10min, x='Month_Sin', bins=50, kde=True, ax=ax[0,2], color='brown')\n",
    "ax[0,2].set_title('Histogram of Month_Sin')\n",
    "sns.histplot(data=df_merged_10min, x='DayOfYear_Sin', bins=50, kde=True, ax=ax[0,3], color='cyan')\n",
    "ax[0,3].set_title('Histogram of DayOfYear_Sin')\n",
    "sns.histplot(data=df_merged_10min, x='HourOfDay_Sin', bins=50, kde=True, ax=ax[1,3], color='magenta')\n",
    "ax[1,3].set_title('Histogram of HourOfDay_Sin')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912b027f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do a pairplot to see relationships between features\n",
    "sns.pairplot(pd.DataFrame(df_merged_10min_normalized, columns=['PV(W)', 'Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', 'Condition Code','Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']))\n",
    "plt.suptitle('Pairplot of Merged 10-Minute Data Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ee608a",
   "metadata": {},
   "source": [
    "### Apply random forrest\n",
    "\n",
    "[SCIKIT Learn - Random Forrest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)  \n",
    "\n",
    "[Shelf learning - Random Forrect](https://shelf.io/blog/random-forests-in-machine-learning/)   \n",
    "\n",
    "[Google - Random Forrest](https://developers.google.com/machine-learning/decision-forests/random-forests)\n",
    "\n",
    "[SCIKIT Learn - train_test_Split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82db8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', 'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']\n",
    "target_col = 'PV(W)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c6e640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do random forest regression to predict PV(W) from the other columns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "# Define features and target variable\n",
    "# exclude PV(W) which is close to zero most of the time\n",
    "threshold = 0.001\n",
    "mask = df_merged_10min_normalized['PV(W)'] > threshold\n",
    "X = df_merged_10min_normalized.loc[mask, feature_cols]\n",
    "y = df_merged_10min_normalized.loc[mask, target_col]\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Create a Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42,max_depth=16,min_samples_split=5,min_samples_leaf=2)\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "# Make predictions\n",
    "y_pred = rf_model.predict(X_test)\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "print(f\"R^2 Score: {r2}\")\n",
    "\n",
    "# Calculate MAPE , ignore division by zero warnings\n",
    "import numpy as np  \n",
    "# Exclude values below a threshold (e.g., 0.01)\n",
    "threshold = 0.01\n",
    "mask = np.abs(y_test) > threshold\n",
    "y_test_filtered = y_test[mask]\n",
    "y_pred_filtered = y_pred[mask]\n",
    "print(f\"Number of samples used for MAPE calculation: {len(y_test_filtered)}\")\n",
    "print(f\"Total samples in y_test: {len(y_test)}\")\n",
    "mape = np.mean(np.abs((y_test_filtered - y_pred_filtered) / y_test_filtered)) * 100\n",
    "print(f\"MAPE (threshold={threshold}): {mape:.2f}%\")\n",
    "print(f\"Mean Absolute Percentage Error (MAPE): {mape} %\")\n",
    "# ...existing code...\n",
    "# Feature importance\n",
    "importances = rf_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "for feature, importance in zip(feature_names, importances):\n",
    "    print(f\"Feature: {feature}, Importance: {importance}\")  \n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(feature_names, importances)\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Feature Importance from Random Forest Regressor')\n",
    "# plot the test zeros vs predicted\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.xlabel('Actual PV(W) (Normalized)')\n",
    "plt.ylabel('Predicted PV(W) (Normalized)')\n",
    "plt.title('Actual vs Predicted PV(W)')\n",
    "# plot a diagonal line\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')\n",
    "# plot standard deviation lines diagonally\n",
    "std_dev = np.std(y_test - y_pred)\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test) + std_dev, max(y_test) + std_dev], color='orange', \n",
    "         linestyle='--', label='+1 Std Dev')\n",
    "plt.plot([min(y_test), max(y_test)], [min(y_test) - std_dev, max(y_test) - std_dev], color='orange', \n",
    "         linestyle='--', label='-1 Std Dev')\n",
    "# plot a line for the test threshold horizontal\n",
    "plt.axvline(x=threshold, color='green', linestyle='--', label='Threshold Line')\n",
    "plt.legend()\n",
    "# Plot error distribution\n",
    "errors = y_test - y_pred\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(errors, bins=50, kde=True)\n",
    "plt.show()\n",
    "# how good is the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb73efc",
   "metadata": {},
   "source": [
    "Now convert back to real values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797e4798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the normalized predictions back to original scale\n",
    "y_test_original = y_test * (df_merged_10min['PV(W)'].max() - df_merged_10min['PV(W)'].min()) + df_merged_10min['PV(W)'].min()\n",
    "y_pred_original = y_pred * (df_merged_10min['PV(W)'].max() - df_merged_10min['PV(W)'].min()) + df_merged_10min['PV(W)'].min()\n",
    "# calculate MAPE on original scale\n",
    "threshold_original = 0.01 * (df_merged_10min['PV(W)'].max() - df_merged_10min['PV(W)'].min()) + df_merged_10min['PV(W)'].min()\n",
    "mask_original = np.abs(y_test_original) > threshold_original\n",
    "y_test_original_filtered = y_test_original[mask_original]\n",
    "y_pred_original_filtered = y_pred_original[mask_original]\n",
    "mape_original = np.mean(np.abs((y_test_original_filtered - y_pred_original_filtered) / y_test_original_filtered)) * 100\n",
    "\n",
    "print(f\"MAPE on original scale (threshold={threshold_original}): {mape_original:.2f}%\")\n",
    "# scatter plot of actual vs predicted on original scale\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(y_test_original, y_pred_original, alpha=0.5)\n",
    "plt.xlabel('Actual PV(W) (Original Scale)')\n",
    "plt.ylabel('Predicted PV(W) (Original Scale)')\n",
    "plt.title('Actual vs Predicted PV(W) on Original Scale')\n",
    "# plot a diagonal line\n",
    "plt.plot([min(y_test_original), max(y_test_original)], [min(y_test_original), max(y_test_original)], color='red', linestyle='--')\n",
    "# Plot error distribution on original scale\n",
    "errors_original = y_test_original - y_pred_original\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.histplot(errors_original, bins=50, kde=True)\n",
    "# plot histogram of errors\n",
    "plt.xlabel('Prediction Error (Original Scale)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Prediction Errors on Original Scale')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "# plot error distribution as a wisker plot on original scale\n",
    "plt.figure(figsize=(8, 3))\n",
    "sns.boxplot(x=errors_original)\n",
    "plt.xlabel('Prediction Error (Original Scale)')\n",
    "plt.title('Boxplot of Prediction Errors on Original Scale')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fbf3e89",
   "metadata": {},
   "source": [
    "### Now try xboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75d343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22eeff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# 1. Define Features (X) and Target (y)\n",
    "# Dropping 'DateTime' as XGBoost needs numerical inputs, and 'PV(W)' because it's the target\n",
    "features = ['Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', \n",
    "            'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']\n",
    "target = 'PV(W)'\n",
    "\n",
    "X = df_merged_10min[features]\n",
    "y = df_merged_10min[target]\n",
    "\n",
    "# 2. Split into Train and Test sets (80% Train, 20% Test)\n",
    "# random_state ensures reproducibility\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training samples: {X_train.shape[0]}\")\n",
    "print(f\"Testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Initialize XGBoost Regressor\n",
    "# n_estimators: Number of boosting rounds (trees)\n",
    "# learning_rate: Step size shrinkage used to prevent overfitting\n",
    "# max_depth: Maximum depth of a tree\n",
    "xg_reg = xgb.XGBRegressor(objective='reg:squarederror', \n",
    "                          n_estimators=100, \n",
    "                          learning_rate=0.1, \n",
    "                          max_depth=5, \n",
    "                          random_state=42)\n",
    "\n",
    "# 4. Fit the model\n",
    "print(\"Training XGBoost model...\")\n",
    "xg_reg.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d5876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Make Predictions\n",
    "y_pred = xg_reg.predict(X_test)\n",
    "\n",
    "# 6. Evaluate Performance\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f} W\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae:.2f} W\")\n",
    "\n",
    "# 7. Visualize Actual vs Predicted (First 100 test points for clarity)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test.values[:100], label='Actual PV(W)', color='blue', alpha=0.7)\n",
    "plt.plot(y_pred[:100], label='Predicted PV(W)', color='red', linestyle='--', alpha=0.7)\n",
    "plt.title('XGBoost: Actual vs Predicted Solar Output (First 100 Test Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Power (W)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 8. Feature Importance Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "xgb.plot_importance(xg_reg, max_num_features=10)\n",
    "plt.title('XGBoost Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21dfc81",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ad10e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4dde47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# 1. Setup Data (Same as XGBoost)\n",
    "# We use the same features and target\n",
    "features = ['Temperature(C)', 'Humidity(%)', 'Solar Radiation(W/m^2)', \n",
    "            'Condition Code', 'Month_Sin', 'DayOfYear_Sin', 'HourOfDay_Sin']\n",
    "target = 'PV(W)'\n",
    "\n",
    "X = df_merged_10min[features]\n",
    "y = df_merged_10min[target]\n",
    "\n",
    "# Split data (using the same seed '42' ensures we compare apples-to-apples with XGBoost)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# 2. Initialize LightGBM Regressor\n",
    "# n_estimators: Number of trees\n",
    "# learning_rate: Speed of learning\n",
    "# num_leaves: Main parameter to control complexity (LightGBM grows leaf-wise)\n",
    "lgbm_reg = LGBMRegressor(n_estimators=100,\n",
    "                         learning_rate=0.1,\n",
    "                         num_leaves=31,  # Default is 31\n",
    "                         random_state=42,\n",
    "                         n_jobs=-1)      # Use all CPU cores\n",
    "\n",
    "# 3. Fit the model\n",
    "print(\"Training LightGBM model...\")\n",
    "lgbm_reg.fit(X_train, y_train)\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e6e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Make Predictions\n",
    "y_pred_lgbm = lgbm_reg.predict(X_test)\n",
    "\n",
    "# 5. Calculate Errors\n",
    "rmse_lgbm = np.sqrt(mean_squared_error(y_test, y_pred_lgbm))\n",
    "mae_lgbm = mean_absolute_error(y_test, y_pred_lgbm)\n",
    "\n",
    "print(f\"LightGBM RMSE: {rmse_lgbm:.2f} W\")\n",
    "print(f\"LightGBM MAE: {mae_lgbm:.2f} W\")\n",
    "\n",
    "# 6. Compare Actual vs Predicted (First 100 samples)\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(y_test.values[:100], label='Actual PV(W)', color='blue', alpha=0.6)\n",
    "plt.plot(y_pred_lgbm[:100], label='LightGBM Prediction', color='green', linestyle='--', alpha=0.8)\n",
    "plt.title('LightGBM: Actual vs Predicted Solar Output (First 100 Test Samples)')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Power (W)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7. Feature Importance\n",
    "# LightGBM provides a convenient plot_importance function\n",
    "plt.figure(figsize=(10, 6))\n",
    "lgb.plot_importance(lgbm_reg, max_num_features=10, importance_type='split')\n",
    "plt.title('LightGBM Feature Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dfc620",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e005d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the database connection\n",
    "sys_cur.close()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
