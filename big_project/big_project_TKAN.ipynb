{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "title",
      "metadata": {},
      "source": [
        "# Temporal Kolmogorov-Arnold Networks (TKAN) for Solar Power Prediction\n",
        "\n",
        "**Author:** Clyde Watts  \n",
        "**Lecturer:** Andrew Beaty  \n",
        "**Date:** 2025-01-11\n",
        "\n",
        "## Overview\n",
        "\n",
        "This notebook implements **Temporal KAN (TKAN)** using **PyTorch** (not Keras/TensorFlow) for solar power prediction.\n",
        "\n",
        "### Key Features:\n",
        "- **Pure PyTorch implementation** - No Keras or TensorFlow\n",
        "- **Temporal encoding** - Explicit use of DateTime column features (hour, day_of_week, day_of_year, month)\n",
        "- **Sinusoidal temporal embeddings** - Similar to positional encoding in Transformers\n",
        "- **Learnable activation functions** - KAN's signature feature with RBF kernels\n",
        "- **Regularization** - L1 regularization on learnable weights\n",
        "\n",
        "### Comparison with Efficient KAN:\n",
        "- Efficient KAN treats time implicitly through features\n",
        "- TKAN explicitly encodes temporal patterns from DateTime column\n",
        "- TKAN uses PyTorch (this notebook), Efficient KAN also uses PyTorch\n",
        "- Both use learnable univariate functions (KAN's core concept)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "imports-header",
      "metadata": {},
      "source": [
        "## Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "id": "imports",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# Import our TKAN implementation\n",
        "from tkan_implementation import TKAN, extract_temporal_features\n",
        "\n",
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "config-header",
      "metadata": {},
      "source": [
        "## Configuration and Paths"
      ]
    },
    {
      "cell_type": "code",
      "id": "config",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Directory paths\n",
        "NOTEBOOK_DIR = os.getcwd()\n",
        "DATA_DIR = os.path.join(NOTEBOOK_DIR, 'data')\n",
        "TRAIN_DATA_DIR = os.path.join(DATA_DIR, 'training_data')\n",
        "MODEL_DIR = os.path.join(NOTEBOOK_DIR, 'model')\n",
        "RESULTS_DIR = os.path.join(NOTEBOOK_DIR, 'results')\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "# Solar configuration\n",
        "NUM_PANELS = 19\n",
        "TOTAL_CAPACITY = 8360  # Watts\n",
        "hourly_nighlty_threshold = 50\n",
        "\n",
        "# Target column\n",
        "target_col = 'PW(V)'\n",
        "\n",
        "print(f'Notebook directory: {NOTEBOOK_DIR}')\n",
        "print(f'Using {NUM_PANELS} panels with {TOTAL_CAPACITY}W capacity')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "load-header",
      "metadata": {},
      "source": [
        "## Load and Prepare Data\n",
        "\n",
        "We load the hourly solar data which includes:\n",
        "- **DateTime**: timestamp for each observation  \n",
        "- **Weather features**: Temperature, Humidity, Wind Speed, etc.\n",
        "- **Solar features**: Clear sky GHI, BHI, etc.\n",
        "- **Target**: PW(V) - solar power output\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "load-data",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Load data\n",
        "df_merge_hourly = pd.read_feather(f\"{TRAIN_DATA_DIR}/hourly_solar_full_data.feather\")\n",
        "\n",
        "# Remove nighttime data\n",
        "df_merge_hourly = df_merge_hourly[df_merge_hourly['Clear sky GHI'] > hourly_nighlty_threshold]\n",
        "\n",
        "# Ensure DateTime is parsed correctly\n",
        "df_merge_hourly['DateTime'] = pd.to_datetime(df_merge_hourly['DateTime'])\n",
        "\n",
        "print(f'Data shape: {df_merge_hourly.shape}')\n",
        "print(f'Date range: {df_merge_hourly[\"DateTime\"].min()} to {df_merge_hourly[\"DateTime\"].max()}')\n",
        "print(f'\\nFirst few rows:')\n",
        "df_merge_hourly.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "features-header",
      "metadata": {},
      "source": [
        "## Feature Selection\n",
        "\n",
        "Select features for the model. Note that we **do NOT** include temporal features\n",
        "in the feature list because TKAN will extract them automatically from the DateTime column.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "features",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Select features (excluding DateTime - we'll extract temporal features separately)\n",
        "feature_cols = [\n",
        "    'Temperature(C)',\n",
        "    'Humidity(%)',\n",
        "    'Precipitation(mm)',\n",
        "    'Dew Point(C)',\n",
        "    'Wind Speed(m/s)',\n",
        "    'Wind Gust(m/s)',\n",
        "    'Pressure(hPa)',\n",
        "    'Wind Cooling',\n",
        "    'Clear sky GHI',\n",
        "    'Clear sky BHI'\n",
        "]\n",
        "\n",
        "print(f'Using {len(feature_cols)} features:')\n",
        "for i, col in enumerate(feature_cols, 1):\n",
        "    print(f'  {i}. {col}')\n",
        "\n",
        "# Extract features and target\n",
        "X = df_merge_hourly[feature_cols].values\n",
        "y = df_merge_hourly[target_col].values.reshape(-1, 1)\n",
        "\n",
        "print(f'\\nX shape: {X.shape}')\n",
        "print(f'y shape: {y.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "split-header",
      "metadata": {},
      "source": [
        "## Train-Test Split\n",
        "\n",
        "**Important:** For time series data, we should split chronologically to avoid data leakage.\n",
        "However, for initial testing, we use random split. In production, use chronological split.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "split",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Split data (80/20)\n",
        "test_size = 0.2\n",
        "random_state = 42\n",
        "\n",
        "X_train, X_test, y_train, y_test, idx_train, idx_test = train_test_split(\n",
        "    X, y, df_merge_hourly.index, test_size=test_size, random_state=random_state\n",
        ")\n",
        "\n",
        "print(f'Train set: {X_train.shape[0]} samples')\n",
        "print(f'Test set: {X_test.shape[0]} samples')\n",
        "\n",
        "# Get DateTime for train and test sets\n",
        "df_train = df_merge_hourly.loc[idx_train]\n",
        "df_test = df_merge_hourly.loc[idx_test]\n",
        "\n",
        "print(f'\\nTrain date range: {df_train[\"DateTime\"].min()} to {df_train[\"DateTime\"].max()}')\n",
        "print(f'Test date range: {df_test[\"DateTime\"].min()} to {df_test[\"DateTime\"].max()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "scaling-header",
      "metadata": {},
      "source": [
        "## Feature Scaling"
      ]
    },
    {
      "cell_type": "code",
      "id": "scaling",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Scale features\n",
        "scaler_X = StandardScaler()\n",
        "scaler_y = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler_X.fit_transform(X_train)\n",
        "X_test_scaled = scaler_X.transform(X_test)\n",
        "\n",
        "y_train_scaled = scaler_y.fit_transform(y_train)\n",
        "y_test_scaled = scaler_y.transform(y_test)\n",
        "\n",
        "print('Data scaled successfully')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "temporal-header",
      "metadata": {},
      "source": [
        "## Extract Temporal Features from DateTime Column\n",
        "\n",
        "This is the key difference from standard KAN!\n",
        "\n",
        "We extract:\n",
        "- **hour**: Hour of day (0-23)\n",
        "- **day_of_week**: Day of week (0=Monday, 6=Sunday)\n",
        "- **day_of_year**: Day of year (1-365)\n",
        "- **month**: Month (1-12)\n",
        "\n",
        "These will be encoded as sinusoidal functions in the TKAN layer.\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "temporal",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Extract temporal features\n",
        "temporal_train = extract_temporal_features(df_train, datetime_col='DateTime')\n",
        "temporal_test = extract_temporal_features(df_test, datetime_col='DateTime')\n",
        "\n",
        "print('Temporal features extracted:')\n",
        "for key, val in temporal_train.items():\n",
        "    print(f'  {key}: shape {val.shape}, range [{val.min():.1f}, {val.max():.1f}]')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tensor-header",
      "metadata": {},
      "source": [
        "## Convert to PyTorch Tensors"
      ]
    },
    {
      "cell_type": "code",
      "id": "tensors",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Convert to PyTorch tensors\n",
        "X_train_tensor = torch.FloatTensor(X_train_scaled).to(device)\n",
        "y_train_tensor = torch.FloatTensor(y_train_scaled).to(device)\n",
        "X_test_tensor = torch.FloatTensor(X_test_scaled).to(device)\n",
        "y_test_tensor = torch.FloatTensor(y_test_scaled).to(device)\n",
        "\n",
        "# Move temporal features to device\n",
        "for key in temporal_train:\n",
        "    temporal_train[key] = temporal_train[key].to(device)\n",
        "    temporal_test[key] = temporal_test[key].to(device)\n",
        "\n",
        "print(f'All tensors moved to {device}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "model-header",
      "metadata": {},
      "source": [
        "## Define TKAN Model\n",
        "\n",
        "Architecture:\n",
        "- **Input layer**: weather/solar features + temporal encoding (16 dimensions)\n",
        "- **Hidden layer**: 32 neurons\n",
        "- **Output layer**: 1 neuron (power prediction)\n",
        "\n",
        "Key parameters:\n",
        "- `grid_size=10`: Number of grid points for learnable functions\n",
        "- `use_temporal=True`: Enable temporal encoding\n",
        "- `temporal_dim=16`: Dimension of temporal encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "model-def",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Define model architecture\n",
        "input_dim = X_train.shape[1]  # Number of weather/solar features\n",
        "hidden_dim = 32\n",
        "output_dim = 1\n",
        "\n",
        "# Create TKAN model\n",
        "model = TKAN(\n",
        "    layers_hidden=[input_dim, hidden_dim, output_dim],\n",
        "    grid_size=10,\n",
        "    use_temporal=True,\n",
        "    temporal_dim=16\n",
        ").to(device)\n",
        "\n",
        "# Count parameters\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'Model: TKAN')\n",
        "print(f'Architecture: {input_dim} -> {hidden_dim} -> {output_dim}')\n",
        "print(f'Total parameters: {total_params:,}')\n",
        "print(f'Trainable parameters: {trainable_params:,}')\n",
        "print(f'\\nModel structure:')\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training-setup-header",
      "metadata": {},
      "source": [
        "## Training Setup\n",
        "\n",
        "Loss function: MSE (Mean Squared Error)  \n",
        "Optimizer: Adam  \n",
        "Learning rate: 0.001  \n",
        "Regularization: L1 on weights (lambda=0.001)\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "training-setup",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Training configuration\n",
        "num_epochs = 100\n",
        "learning_rate = 0.001\n",
        "reg_lambda = 0.001\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Track losses\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "\n",
        "print(f'Training configuration:')\n",
        "print(f'  Epochs: {num_epochs}')\n",
        "print(f'  Learning rate: {learning_rate}')\n",
        "print(f'  Regularization lambda: {reg_lambda}')\n",
        "print(f'  Optimizer: Adam')\n",
        "print(f'  Criterion: MSE')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "training-header",
      "metadata": {},
      "source": [
        "## Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "id": "training",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "print('Starting training...')\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    # Forward pass with temporal features\n",
        "    y_pred_train = model(X_train_tensor, temporal_train)\n",
        "    \n",
        "    # Compute loss\n",
        "    loss = criterion(y_pred_train, y_train_tensor)\n",
        "    \n",
        "    # Add regularization\n",
        "    reg_loss = model.regularization_loss()\n",
        "    total_loss = loss + reg_lambda * reg_loss\n",
        "    \n",
        "    # Backward pass\n",
        "    total_loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_losses.append(loss.item())\n",
        "    \n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        y_pred_test = model(X_test_tensor, temporal_test)\n",
        "        test_loss = criterion(y_pred_test, y_test_tensor)\n",
        "        test_losses.append(test_loss.item())\n",
        "    \n",
        "    # Print progress\n",
        "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}] '\n",
        "              f'Train Loss: {loss.item():.6f} '\n",
        "              f'Test Loss: {test_loss.item():.6f} '\n",
        "              f'Reg Loss: {reg_loss.item():.6f}')\n",
        "\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f'\\nTraining completed in {elapsed_time:.2f} seconds')\n",
        "print(f'Final train loss: {train_losses[-1]:.6f}')\n",
        "print(f'Final test loss: {test_losses[-1]:.6f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "curves-header",
      "metadata": {},
      "source": [
        "## Training Curves"
      ]
    },
    {
      "cell_type": "code",
      "id": "curves",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Plot training and test loss\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss', alpha=0.7)\n",
        "plt.plot(test_losses, label='Test Loss', alpha=0.7)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSE Loss')\n",
        "plt.title('TKAN Training History')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f'Minimum train loss: {min(train_losses):.6f} at epoch {train_losses.index(min(train_losses))+1}')\n",
        "print(f'Minimum test loss: {min(test_losses):.6f} at epoch {test_losses.index(min(test_losses))+1}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "predictions-header",
      "metadata": {},
      "source": [
        "## Make Predictions"
      ]
    },
    {
      "cell_type": "code",
      "id": "predictions",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Make predictions\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    y_pred_train_scaled = model(X_train_tensor, temporal_train)\n",
        "    y_pred_test_scaled = model(X_test_tensor, temporal_test)\n",
        "\n",
        "# Convert to numpy and inverse transform\n",
        "y_pred_train = scaler_y.inverse_transform(y_pred_train_scaled.cpu().numpy())\n",
        "y_pred_test = scaler_y.inverse_transform(y_pred_test_scaled.cpu().numpy())\n",
        "\n",
        "print('Predictions generated successfully')\n",
        "print(f'Train predictions shape: {y_pred_train.shape}')\n",
        "print(f'Test predictions shape: {y_pred_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "metrics-header",
      "metadata": {},
      "source": [
        "## Evaluation Metrics\n",
        "\n",
        "Compute standard regression metrics:\n",
        "- RMSE (Root Mean Squared Error)\n",
        "- MAE (Mean Absolute Error)\n",
        "- R\u00b2 Score\n"
      ]
    },
    {
      "cell_type": "code",
      "id": "metrics",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compute metrics\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {'rmse': rmse, 'mae': mae, 'r2': r2}\n",
        "\n",
        "train_metrics = compute_metrics(y_train, y_pred_train)\n",
        "test_metrics = compute_metrics(y_test, y_pred_test)\n",
        "\n",
        "print('=' * 60)\n",
        "print('TKAN Model Performance')\n",
        "print('=' * 60)\n",
        "print(f'\\nTrain Set:')\n",
        "print(f'  RMSE: {train_metrics[\"rmse\"]:.4f}')\n",
        "print(f'  MAE:  {train_metrics[\"mae\"]:.4f}')\n",
        "print(f'  R\u00b2:   {train_metrics[\"r2\"]:.4f}')\n",
        "\n",
        "print(f'\\nTest Set:')\n",
        "print(f'  RMSE: {test_metrics[\"rmse\"]:.4f}')\n",
        "print(f'  MAE:  {test_metrics[\"mae\"]:.4f}')\n",
        "print(f'  R\u00b2:   {test_metrics[\"r2\"]:.4f}')\n",
        "print('=' * 60)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "viz-header",
      "metadata": {},
      "source": [
        "## Visualization: Predictions vs Actual"
      ]
    },
    {
      "cell_type": "code",
      "id": "viz-scatter",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Scatter plot: Predictions vs Actual\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Train set\n",
        "axes[0].scatter(y_train, y_pred_train, alpha=0.5, s=10)\n",
        "axes[0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
        "axes[0].set_xlabel('Actual PW(V)')\n",
        "axes[0].set_ylabel('Predicted PW(V)')\n",
        "axes[0].set_title(f'Train Set (R\u00b2={train_metrics[\"r2\"]:.4f})')\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# Test set\n",
        "axes[1].scatter(y_test, y_pred_test, alpha=0.5, s=10, color='orange')\n",
        "axes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
        "axes[1].set_xlabel('Actual PW(V)')\n",
        "axes[1].set_ylabel('Predicted PW(V)')\n",
        "axes[1].set_title(f'Test Set (R\u00b2={test_metrics[\"r2\"]:.4f})')\n",
        "axes[1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "residuals-header",
      "metadata": {},
      "source": [
        "## Residual Analysis"
      ]
    },
    {
      "cell_type": "code",
      "id": "residuals",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Compute residuals\n",
        "train_residuals = y_train.flatten() - y_pred_train.flatten()\n",
        "test_residuals = y_test.flatten() - y_pred_test.flatten()\n",
        "\n",
        "# Plot residuals\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Residuals vs Predicted\n",
        "axes[0, 0].scatter(y_pred_train, train_residuals, alpha=0.5, s=10)\n",
        "axes[0, 0].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0, 0].set_xlabel('Predicted PW(V)')\n",
        "axes[0, 0].set_ylabel('Residuals')\n",
        "axes[0, 0].set_title('Train Set Residuals')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[0, 1].scatter(y_pred_test, test_residuals, alpha=0.5, s=10, color='orange')\n",
        "axes[0, 1].axhline(y=0, color='r', linestyle='--')\n",
        "axes[0, 1].set_xlabel('Predicted PW(V)')\n",
        "axes[0, 1].set_ylabel('Residuals')\n",
        "axes[0, 1].set_title('Test Set Residuals')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# Residual histograms\n",
        "axes[1, 0].hist(train_residuals, bins=50, alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_xlabel('Residuals')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "axes[1, 0].set_title('Train Residuals Distribution')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "axes[1, 1].hist(test_residuals, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
        "axes[1, 1].set_xlabel('Residuals')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].set_title('Test Residuals Distribution')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "save-header",
      "metadata": {},
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "id": "save",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Save model\n",
        "model_path = os.path.join(MODEL_DIR, 'tkan_model.pth')\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'train_losses': train_losses,\n",
        "    'test_losses': test_losses,\n",
        "    'train_metrics': train_metrics,\n",
        "    'test_metrics': test_metrics,\n",
        "    'feature_cols': feature_cols,\n",
        "    'scaler_X': scaler_X,\n",
        "    'scaler_y': scaler_y\n",
        "}, model_path)\n",
        "\n",
        "print(f'Model saved to: {model_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary-header",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "### What We Did:\n",
        "\n",
        "1. **Implemented TKAN** - A Temporal KAN using pure PyTorch\n",
        "2. **Temporal Encoding** - Extracted hour, day_of_week, day_of_year, month from DateTime column\n",
        "3. **Sinusoidal Embeddings** - Encoded temporal features as sin/cos functions\n",
        "4. **Learnable Activations** - Used RBF kernels for KAN's characteristic learnable functions\n",
        "5. **Training** - Trained with MSE loss + L1 regularization\n",
        "6. **Evaluation** - Computed RMSE, MAE, R\u00b2 metrics\n",
        "\n",
        "### Key Differences from Efficient KAN:\n",
        "\n",
        "- **Explicit temporal encoding**: TKAN uses DateTime column explicitly\n",
        "- **Sinusoidal embeddings**: Similar to positional encoding in Transformers\n",
        "- **Both use PyTorch**: This eliminates Keras/TensorFlow dependency\n",
        "- **RBF-based KAN**: Simpler than B-splines but captures similar patterns\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "- Compare with Efficient KAN results\n",
        "- Try different temporal_dim values (8, 16, 32)\n",
        "- Experiment with grid_size\n",
        "- Add more hidden layers\n",
        "- Use chronological split for more realistic evaluation\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}